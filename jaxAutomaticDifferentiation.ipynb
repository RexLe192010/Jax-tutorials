{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Taking gradients with jax.grad"
      ],
      "metadata": {
        "id": "MdoixVuQd-6C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import grad\n",
        "\n",
        "grad_tanh = grad(jnp.tanh)\n",
        "print(grad_tanh(2.0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQgvIMvOeC4C",
        "outputId": "c0ea6e17-9675-48ba-c86f-cf3f50d00335"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.070650816\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since jax.grad() operates on functions, you can apply it to its own output to differentiate as many times as you like:"
      ],
      "metadata": {
        "id": "eFYcniWgePCC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(grad(grad(jnp.tanh))(2.0))\n",
        "print(grad(grad(grad(jnp.tanh)))(2.0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VF-GRkTJePcS",
        "outputId": "5c1d473d-a13f-4c50-89e5-e6e8a76addb0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-0.13621868\n",
            "0.25265405\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "JAX’s autodiff makes it easy to compute higher-order derivatives, because the functions that compute derivatives are themselves differentiable. Thus, higher-order derivatives are as easy as stacking transformations. This can be illustrated in the single-variable case:\n",
        "\n",
        "The derivative of f(x)=x^3+2x^2-3x+1\n",
        " can be computed as:"
      ],
      "metadata": {
        "id": "Nxi6ioSbeWkp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f = lambda x: x**3 + 2*x**2 - 3*x + 1\n",
        "dfdx = jax.grad(f)"
      ],
      "metadata": {
        "id": "_RMkCkiYec9a"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Computing any of these in JAX is as easy as chaining the jax.grad() function:"
      ],
      "metadata": {
        "id": "ROyzjDxgeiHi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d2fdx = jax.grad(dfdx)\n",
        "d3fdx = jax.grad(d2fdx)\n",
        "d4fdx = jax.grad(d3fdx)\n",
        "\n",
        "# Evaluation\n",
        "print(dfdx(1.))\n",
        "print(d2fdx(1.))\n",
        "print(d3fdx(1.))\n",
        "print(d4fdx(1.))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhn4PlJkej0C",
        "outputId": "79af5b0c-d9fc-4983-91f7-70b68db97778"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.0\n",
            "10.0\n",
            "6.0\n",
            "0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Computing gradients in a linear logistic regression"
      ],
      "metadata": {
        "id": "6gfRPmBferAC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next example shows how to compute gradients with jax.grad() in a linear logistic regression model. First, the setup:"
      ],
      "metadata": {
        "id": "JvCvntnyexFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "key = jax.random.key(0)\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 0.5 * (jnp.tanh(x / 2) + 1)\n",
        "\n",
        "# Outputs probability of a label being true.\n",
        "def predict(W, b, inputs):\n",
        "  return sigmoid(jnp.dot(inputs, W) + b)\n",
        "\n",
        "# Build a toy dataset.\n",
        "inputs = jnp.array([[0.52, 1.12,  0.77],\n",
        "                    [0.88, -1.08, 0.15],\n",
        "                    [0.52, 0.06, -1.30],\n",
        "                    [0.74, -2.49, 1.39]])\n",
        "targets = jnp.array([True, True, False, True])\n",
        "\n",
        "# Training loss is the negative log-likelihood of the training examples.\n",
        "def loss(W, b):\n",
        "  preds = predict(W, b, inputs)\n",
        "  label_probs = preds * targets + (1 - preds) * (1 - targets)\n",
        "  return -jnp.sum(jnp.log(label_probs))\n",
        "\n",
        "# Initialize random model coefficients\n",
        "key, W_key, b_key = jax.random.split(key, 3)\n",
        "W = jax.random.normal(W_key, (3,))\n",
        "b = jax.random.normal(b_key, ())"
      ],
      "metadata": {
        "id": "yihUx97oet_C"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the jax.grad() function with its argnums argument to differentiate a function with respect to positional arguments."
      ],
      "metadata": {
        "id": "HPPadcy_nAYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Differentiate `loss` with respect to the first positional argument:\n",
        "W_grad = grad(loss, argnums=0)(W, b)\n",
        "print(f'{W_grad=}')\n",
        "\n",
        "# Since argnums=0 is the default, this does the same thing:\n",
        "W_grad = grad(loss)(W, b)\n",
        "print(f'{W_grad=}')\n",
        "\n",
        "# But you can choose different values too, and drop the keyword:\n",
        "b_grad = grad(loss, 1)(W, b)\n",
        "print(f'{b_grad=}')\n",
        "\n",
        "# Including tuple values\n",
        "W_grad, b_grad = grad(loss, (0, 1))(W, b)\n",
        "print(f'{W_grad=}')\n",
        "print(f'{b_grad=}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dP9VpdFUnAue",
        "outputId": "442a65b9-000a-4ff2-9760-a2239e951eb7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W_grad=Array([-0.43314594, -0.7354604 , -1.2598921 ], dtype=float32)\n",
            "W_grad=Array([-0.43314594, -0.7354604 , -1.2598921 ], dtype=float32)\n",
            "b_grad=Array(-0.69001764, dtype=float32)\n",
            "W_grad=Array([-0.43314594, -0.7354604 , -1.2598921 ], dtype=float32)\n",
            "b_grad=Array(-0.69001764, dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Differentiating with respect to nested lists, tuples, and dicts"
      ],
      "metadata": {
        "id": "ERN13_6Fn7o9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Due to JAX’s PyTree abstraction (see Working with pytrees), differentiating with respect to standard Python containers just works, so use tuples, lists, and dicts (and arbitrary nesting) however you like.\n",
        "\n",
        "Continuing the previous example:"
      ],
      "metadata": {
        "id": "aopV3K4PoO3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss2(params_dict):\n",
        "    preds = predict(params_dict['W'], params_dict['b'], inputs)\n",
        "    label_probs = preds * targets + (1 - preds) * (1 - targets)\n",
        "    return -jnp.sum(jnp.log(label_probs))\n",
        "\n",
        "print(grad(loss2)({'W': W, 'b': b}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lWcZrTGoPU1",
        "outputId": "98a98e42-75e6-4872-eb48-d506a29a547b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'W': Array([-0.43314594, -0.7354604 , -1.2598921 ], dtype=float32), 'b': Array(-0.69001764, dtype=float32)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can create Custom pytree nodes to work with not just jax.grad() but other JAX transformations (jax.jit(), jax.vmap(), and so on)."
      ],
      "metadata": {
        "id": "YpORcT_DoSKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating a function and its gradient using jax.value_and_grad"
      ],
      "metadata": {
        "id": "Q2mKG7d8oTuF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another convenient function is jax.value_and_grad() for efficiently computing both a function’s value as well as its gradient’s value in one pass.\n",
        "\n",
        "Continuing the previous examples:"
      ],
      "metadata": {
        "id": "IzIk0A3ToUnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_value, Wb_grad = jax.value_and_grad(loss, (0, 1))(W, b)\n",
        "print('loss value', loss_value)\n",
        "print('loss value', loss(W, b))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGm7YTT1oWFe",
        "outputId": "fe3bcf07-e826-4864-d788-e238b46e6cce"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss value 2.9729187\n",
            "loss value 2.9729187\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checking against numerical differences"
      ],
      "metadata": {
        "id": "yNLHqWyCoeLd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A great thing about derivatives is that they’re straightforward to check with finite differences.\n",
        "\n",
        "Continuing the previous examples:"
      ],
      "metadata": {
        "id": "W30v3lsKof_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set a step size for finite differences calculations\n",
        "eps = 1e-4\n",
        "\n",
        "# Check b_grad with scalar finite differences\n",
        "b_grad_numerical = (loss(W, b + eps / 2.) - loss(W, b - eps / 2.)) / eps\n",
        "print('b_grad_numerical', b_grad_numerical)\n",
        "print('b_grad_autodiff', grad(loss, 1)(W, b))\n",
        "\n",
        "# Check W_grad with finite differences in a random direction\n",
        "key, subkey = jax.random.split(key)\n",
        "vec = jax.random.normal(subkey, W.shape)\n",
        "unitvec = vec / jnp.sqrt(jnp.vdot(vec, vec))\n",
        "W_grad_numerical = (loss(W + eps / 2. * unitvec, b) - loss(W - eps / 2. * unitvec, b)) / eps\n",
        "print('W_dirderiv_numerical', W_grad_numerical)\n",
        "print('W_dirderiv_autodiff', jnp.vdot(grad(loss)(W, b), unitvec))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZZwmOi7og7-",
        "outputId": "477793c5-4571-4f43-d33e-ada24ce1ef50"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b_grad_numerical -0.6890297\n",
            "b_grad_autodiff -0.69001764\n",
            "W_dirderiv_numerical 1.3041496\n",
            "W_dirderiv_autodiff 1.3006743\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "JAX provides a simple convenience function that does essentially the same thing, but checks up to any order of differentiation that you like:"
      ],
      "metadata": {
        "id": "AxBS-yxzpZIV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from jax.test_util import check_grads\n",
        "\n",
        "check_grads(loss, (W, b), order=2)  # check up to 2nd order derivatives"
      ],
      "metadata": {
        "id": "K1L6ag6opZh2"
      },
      "execution_count": 14,
      "outputs": []
    }
  ]
}