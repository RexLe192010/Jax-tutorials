{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to JAX primitives"
      ],
      "metadata": {
        "id": "UEtRNSgyfqMg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A JAX primitive is the basic computational unit of a JAX program. This document explains the interface that a JAX primitive must support to allow JAX to perform all its transformations (this is not a how-to guide).\n",
        "\n",
        "For example, the multiply-add operation can be implemented in terms of the low-level jax.lax.* primitives (which are like XLA operator wrappers) or jax.extend.core.Primitive(\"multiply_add\"), as demonstrated further below.\n",
        "\n",
        "And JAX is able to take sequences of such primitive operations, and transform them via its composable transformations of Python functions, such as jax.jit(), jax.grad() and jax.vmap(). JAX implements these transforms in a JAX-traceable way. This means that when a Python function is executed, the only operations it applies to the data are either:\n",
        "\n",
        "1. Inspections of data attributes: Data information, such as shape or type; or\n",
        "\n",
        "2. JAX primitives: These are the JAX special operations covered in this tutorial.\n",
        "\n",
        "JAX primitives know how to operate on both concrete data values and abstract JAX values. A JAX-traceable function can be invoked by JAX with abstract arguments. For example, a JAX abstract value — ShapedArray(float32[2,2]) — captures the type and the shape of values, but not the concrete data values.\n",
        "\n",
        "The JAX-transformed functions must themselves be JAX-traceable functions to make sure that these transformations are composable, for example like jax.jit(jax.jacfwd(jax.grad(f))).\n",
        "\n",
        "JAX provides pre-defined primitives corresponding to most XLA operations, including add, matmul, sin, cos, and indexing.\n",
        "\n",
        "In addition, JAX offers an implementation of NumPy functions in terms of JAX primitives. This means that Python programs using JAX’s implementation of NumPy are JAX-traceable and, therefore, transformable. Other libraries can be made JAX-traceable by implementing them in terms of JAX primitives.\n",
        "\n",
        "Furthermore, the set of JAX primitives is extensible, so instead of reimplementing a function in terms of pre-defined JAX primitives, you can define a new primitive that encapsulates the behavior of the function.\n",
        "\n",
        "Consider the following example: you want to add to JAX support for a multiply-add function with three arguments, defined mathematically as multiply_add(x, y, z) = x * y + z. This function operates on 3 identically-shaped tensors of floating point values and performs the operations pointwise. You can do this by:\n",
        "\n",
        "1. Using existing JAX primitives; or\n",
        "\n",
        "2. Defining new JAX primitives"
      ],
      "metadata": {
        "id": "gcDpxksuftPv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using existing JAX primitives"
      ],
      "metadata": {
        "id": "hoPsGRxnh-6n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The easiest way to define new functions is to write them in terms of JAX primitives, or in terms of other functions that are themselves written using JAX primitives, for example, those defined in the jax.lax() module:"
      ],
      "metadata": {
        "id": "Ey9w8BNSiAf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from jax import lax\n",
        "from jax._src import api\n",
        "\n",
        "def multiply_add_lax(x, y, z):\n",
        "  \"\"\"Implementation of multiply-add using the `jax.lax` primitives.\"\"\"\n",
        "  return lax.add(lax.mul(x, y), z)\n",
        "\n",
        "\n",
        "def square_add_lax(a, b):\n",
        "  \"\"\"A square-add function using the newly defined multiply-add.\"\"\"\n",
        "  return multiply_add_lax(a, a, b)\n",
        "\n",
        "print(\"square_add_lax = \", square_add_lax(2., 10.))\n",
        "# Differentiate w.r.t. the first argument\n",
        "print(\"grad(square_add_lax) = \", api.grad(square_add_lax, argnums=0)(2.0, 10.))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k28WAmCUkiJF",
        "outputId": "f9cc5067-a95d-4349-a64a-df151299c09b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "square_add_lax =  14.0\n",
            "grad(square_add_lax) =  4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To understand how JAX is internally using the primitives, add some helpers for tracing function calls:"
      ],
      "metadata": {
        "id": "tFnkiYCDkq6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Helper functions (execute this cell)\n",
        "import functools\n",
        "import traceback\n",
        "\n",
        "_indentation = 0\n",
        "def _trace(msg=None):\n",
        "    \"\"\"Print a message at current indentation.\"\"\"\n",
        "    if msg is not None:\n",
        "        print(\"  \" * _indentation + msg)\n",
        "\n",
        "def _trace_indent(msg=None):\n",
        "    \"\"\"Print a message and then indent the rest.\"\"\"\n",
        "    global _indentation\n",
        "    _trace(msg)\n",
        "    _indentation = 1 + _indentation\n",
        "\n",
        "def _trace_unindent(msg=None):\n",
        "    \"\"\"Unindent then print a message.\"\"\"\n",
        "    global _indentation\n",
        "    _indentation = _indentation - 1\n",
        "    _trace(msg)\n",
        "\n",
        "def trace(name):\n",
        "  \"\"\"A decorator for functions to trace arguments and results.\"\"\"\n",
        "\n",
        "  def trace_func(func):  # pylint: disable=missing-docstring\n",
        "    def pp(v):\n",
        "        \"\"\"Print certain values more succinctly\"\"\"\n",
        "        vtype = str(type(v))\n",
        "        if \"jax._src.xla_bridge._JaxComputationBuilder\" in vtype:\n",
        "            return \"<JaxComputationBuilder>\"\n",
        "        elif \"jaxlib._jax_.XlaOp\" in vtype:\n",
        "            return \"<XlaOp at 0x{:x}>\".format(id(v))\n",
        "        elif (\"partial_eval.JaxprTracer\" in vtype or\n",
        "              \"batching.BatchTracer\" in vtype or\n",
        "              \"ad.JVPTracer\" in vtype):\n",
        "            return \"Traced<{}>\".format(v.aval)\n",
        "        elif isinstance(v, tuple):\n",
        "            return \"({})\".format(pp_values(v))\n",
        "        else:\n",
        "            return str(v)\n",
        "    def pp_values(args):\n",
        "        return \", \".join([pp(arg) for arg in args])\n",
        "\n",
        "    @functools.wraps(func)\n",
        "    def func_wrapper(*args):\n",
        "      _trace_indent(\"call {}({})\".format(name, pp_values(args)))\n",
        "      res = func(*args)\n",
        "      _trace_unindent(\"|<- {} = {}\".format(name, pp(res)))\n",
        "      return res\n",
        "\n",
        "    return func_wrapper\n",
        "\n",
        "  return trace_func\n",
        "\n",
        "class expectNotImplementedError(object):\n",
        "  \"\"\"Context manager to check for NotImplementedError.\"\"\"\n",
        "  def __enter__(self): pass\n",
        "  def __exit__(self, type, value, tb):\n",
        "    global _indentation\n",
        "    _indentation = 0\n",
        "    if type is NotImplementedError:\n",
        "      print(\"\\nFound expected exception:\")\n",
        "      traceback.print_exc(limit=3)\n",
        "      return True\n",
        "    elif type is None:  # No exception\n",
        "      assert False, \"Expected NotImplementedError\"\n",
        "    else:\n",
        "      return False"
      ],
      "metadata": {
        "id": "_Pfmt9Zpkr_U"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of using jax.lax() primitives directly, you can use other functions that are already written in terms of those primitives, such as those in jax.numpy:"
      ],
      "metadata": {
        "id": "MIEymvg6nqps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "\n",
        "@trace(\"multiply_add_numpy\")\n",
        "def multiply_add_numpy(x, y, z):\n",
        "    return jnp.add(jnp.multiply(x, y), z)\n",
        "\n",
        "@trace(\"square_add_numpy\")\n",
        "def square_add_numpy(a, b):\n",
        "    return multiply_add_numpy(a, a, b)\n",
        "\n",
        "print(\"\\nNormal evaluation:\")\n",
        "print(\"square_add_numpy = \", square_add_numpy(2., 10.))\n",
        "print(\"\\nGradient evaluation:\")\n",
        "print(\"grad(square_add_numpy) = \", api.grad(square_add_numpy)(2.0, 10.))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ylz4mjOAnrnE",
        "outputId": "cbe201ff-2277-4a0f-f966-66b38e70b3d2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Normal evaluation:\n",
            "call square_add_numpy(2.0, 10.0)\n",
            "  call multiply_add_numpy(2.0, 2.0, 10.0)\n",
            "  |<- multiply_add_numpy = 14.0\n",
            "|<- square_add_numpy = 14.0\n",
            "square_add_numpy =  14.0\n",
            "\n",
            "Gradient evaluation:\n",
            "call square_add_numpy(Traced<ShapedArray(float32[], weak_type=True)>, 10.0)\n",
            "  call multiply_add_numpy(Traced<ShapedArray(float32[], weak_type=True)>, Traced<ShapedArray(float32[], weak_type=True)>, 10.0)\n",
            "  |<- multiply_add_numpy = Traced<ShapedArray(float32[], weak_type=True)>\n",
            "|<- square_add_numpy = Traced<ShapedArray(float32[], weak_type=True)>\n",
            "grad(square_add_numpy) =  4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that in the process of computing jax.grad(), JAX invokes square_add_numpy and multiply_add_numpy with special arguments ConcreteArray(...) (described further below in this colab). It is important to remember that a JAX-traceable function must be able to operate not only on concrete arguments but also on special abstract arguments that JAX may use to abstract the function execution.\n",
        "\n",
        "The JAX traceability property is satisfied as long as the function is written in terms of JAX primitives."
      ],
      "metadata": {
        "id": "DAXSTKGjn1KD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining new JAX primitives"
      ],
      "metadata": {
        "id": "vit8A-pIpuCi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The right way to add support for multiply-add is in terms of existing JAX primitives, as shown above. However, to demonstrate how JAX primitives work, pretend that you want to add a new primitive to JAX for the multiply-add functionality."
      ],
      "metadata": {
        "id": "uAkqHCaepwOK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from jax.extend import core\n",
        "\n",
        "multiply_add_p = core.Primitive(\"multiply_add\")  # Create the primitive\n",
        "\n",
        "@trace(\"multiply_add_prim\")\n",
        "def multiply_add_prim(x, y, z):\n",
        "  \"\"\"The JAX-traceable way to use the JAX primitive.\n",
        "\n",
        "  Note that the traced arguments must be passed as positional arguments\n",
        "  to `bind`.\n",
        "  \"\"\"\n",
        "  return multiply_add_p.bind(x, y, z)\n",
        "\n",
        "@trace(\"square_add_prim\")\n",
        "def square_add_prim(a, b):\n",
        "  \"\"\"A square-add function implemented using the new JAX-primitive.\"\"\"\n",
        "  return multiply_add_prim(a, a, b)"
      ],
      "metadata": {
        "id": "o8_xybGdpxjT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you try to call the newly defined functions, you’ll get an error, because you haven’t yet told JAX anything about the semantics of the new primitive."
      ],
      "metadata": {
        "id": "hhxUY4ffp0CC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with expectNotImplementedError():\n",
        "  square_add_prim(2., 10.)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18cOn5Pop1RS",
        "outputId": "e6af015f-5bc6-4aaf-c109-a4ceda11148c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "call square_add_prim(2.0, 10.0)\n",
            "  call multiply_add_prim(2.0, 2.0, 10.0)\n",
            "\n",
            "Found expected exception:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-5-2844449444.py\", line 2, in <cell line: 0>\n",
            "    square_add_prim(2., 10.)\n",
            "  File \"/tmp/ipython-input-2-2803309189.py\", line 48, in func_wrapper\n",
            "    res = func(*args)\n",
            "          ^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-4-2637569133.py\", line 17, in square_add_prim\n",
            "    return multiply_add_prim(a, a, b)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "NotImplementedError: Evaluation rule for 'multiply_add' not implemented\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Primal evaluation rules"
      ],
      "metadata": {
        "id": "hjpfy8E_p83q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@trace(\"multiply_add_impl\")\n",
        "def multiply_add_impl(x, y, z):\n",
        "  \"\"\"Concrete implementation of the primitive.\n",
        "\n",
        "  This function does not need to be JAX traceable.\n",
        "\n",
        "  Args:\n",
        "    x, y, z: The concrete arguments of the primitive. Will only be called with\n",
        "      concrete values.\n",
        "\n",
        "  Returns:\n",
        "    the concrete result of the primitive.\n",
        "  \"\"\"\n",
        "  # Note: you can use the ordinary (non-JAX) NumPy, which is not JAX-traceable.\n",
        "  return np.add(np.multiply(x, y), z)\n",
        "\n",
        "# Now, register the primal implementation with JAX:\n",
        "multiply_add_p.def_impl(multiply_add_impl)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "id": "NSehzZVpp-cy",
        "outputId": "1e07dd83-1525-49d1-a63b-c3653640642e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function __main__.multiply_add_impl(x, y, z)>"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>multiply_add_impl</b><br/>def multiply_add_impl(x, y, z)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/tmp/ipython-input-6-811635853.py</a>Concrete implementation of the primitive.\n",
              "\n",
              "This function does not need to be JAX traceable.\n",
              "\n",
              "Args:\n",
              "  x, y, z: The concrete arguments of the primitive. Will only be called with \n",
              "    concrete values.\n",
              "\n",
              "Returns:\n",
              "  the concrete result of the primitive.</pre></div>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert square_add_prim(2., 10.) == 14."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Yz24oPnqA-r",
        "outputId": "ca620751-7277-4d06-cb55-abb0ffe32b8a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "call square_add_prim(2.0, 10.0)\n",
            "  call multiply_add_prim(2.0, 2.0, 10.0)\n",
            "    call multiply_add_impl(2.0, 2.0, 10.0)\n",
            "    |<- multiply_add_impl = 14.0\n",
            "  |<- multiply_add_prim = 14.0\n",
            "|<- square_add_prim = 14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What happens when you use jit"
      ],
      "metadata": {
        "id": "g4-7dq5mqDLa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, if you try to use jit, you’ll get a NotImplementedError:"
      ],
      "metadata": {
        "id": "gaHrdJARqGIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with expectNotImplementedError():\n",
        "  api.jit(square_add_prim)(2., 10.)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKkoIc4KqHKC",
        "outputId": "4c48409b-9a31-4299-d171-2f1bb6d98341"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "call square_add_prim(Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace>, Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace>)\n",
            "  call multiply_add_prim(Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace>, Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace>, Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace>)\n",
            "\n",
            "Found expected exception:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-8-1813425700.py\", line 2, in <cell line: 0>\n",
            "    api.jit(square_add_prim)(2., 10.)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/jax/_src/traceback_util.py\", line 180, in reraise_with_filtered_traceback\n",
            "    return fun(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/jax/_src/pjit.py\", line 341, in cache_miss\n",
            "    pgle_profiler) = _python_pjit_helper(fun, jit_info, *args, **kwargs)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "NotImplementedError: Abstract evaluation for 'multiply_add' not implemented\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Abstract evaluation rules"
      ],
      "metadata": {
        "id": "BYVssw05q-kq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To JIT the function, and for other transformations as well, JAX first evaluates it abstractly using only the shape and type of the arguments. This abstract evaluation serves multiple purposes:\n",
        "\n",
        "1. Gets the sequence of JAX primitives that are used in the computation. This sequence will be compiled.\n",
        "\n",
        "2. Computes the shape and type of all vectors and operations used in the computation.\n",
        "\n",
        "For example, the abstraction of a vector with 3 elements may be ShapedArray(float32[3]), or ConcreteArray([1., 2., 3.]). In the latter case, JAX uses the actual concrete value wrapped as an abstract value."
      ],
      "metadata": {
        "id": "fAdm6d8yrBFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from jax import core\n",
        "\n",
        "@trace(\"multiply_add_abstract_eval\")\n",
        "def multiply_add_abstract_eval(xs, ys, zs):\n",
        "  \"\"\"Abstract evaluation of the primitive.\n",
        "\n",
        "  This function does not need to be JAX traceable. It will be invoked with\n",
        "  abstractions of the actual arguments\n",
        "\n",
        "  Args:\n",
        "    xs, ys, zs: Abstractions of the arguments.\n",
        "\n",
        "  Result:\n",
        "    a ShapedArray for the result of the primitive.\n",
        "  \"\"\"\n",
        "  assert xs.shape == ys.shape\n",
        "  assert xs.shape == zs.shape\n",
        "  return core.ShapedArray(xs.shape, xs.dtype)\n",
        "\n",
        "# Now, register the abstract evaluation with JAX:\n",
        "multiply_add_p.def_abstract_eval(multiply_add_abstract_eval)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "id": "MdwVs20trFRj",
        "outputId": "dcf26c04-3470-4a89-8bc9-c605c019364a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function __main__.multiply_add_abstract_eval(xs, ys, zs)>"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>multiply_add_abstract_eval</b><br/>def multiply_add_abstract_eval(xs, ys, zs)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/tmp/ipython-input-9-803205338.py</a>Abstract evaluation of the primitive.\n",
              "\n",
              "This function does not need to be JAX traceable. It will be invoked with\n",
              "abstractions of the actual arguments\n",
              "\n",
              "Args:\n",
              "  xs, ys, zs: Abstractions of the arguments.\n",
              "\n",
              "Result:\n",
              "  a ShapedArray for the result of the primitive.</pre></div>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you re-attempt to apply jit, you can inspect how the abstract evaluation proceeds, but you’ll get another error about missing the actual XLA compilation rule:"
      ],
      "metadata": {
        "id": "vGcHLHSBs-AZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with expectNotImplementedError():\n",
        "  api.jit(square_add_prim)(2., 10.)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYi81Syss_Bh",
        "outputId": "7f6a8f54-f950-4be1-fbd5-44bfed876ffe"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "call square_add_prim(Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace>, Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace>)\n",
            "  call multiply_add_prim(Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace>, Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace>, Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace>)\n",
            "    call multiply_add_abstract_eval(ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True))\n",
            "    |<- multiply_add_abstract_eval = ShapedArray(float32[])\n",
            "  |<- multiply_add_prim = Traced<ShapedArray(float32[])>with<DynamicJaxprTrace>\n",
            "|<- square_add_prim = Traced<ShapedArray(float32[])>with<DynamicJaxprTrace>\n",
            "\n",
            "Found expected exception:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    ColabKernelApp.launch_instance()\n",
            "jax._src.source_info_util.JaxStackTraceBeforeTransformation: NotImplementedError: MLIR translation rule for primitive 'multiply_add' not found for platform cpu\n",
            "\n",
            "The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.\n",
            "\n",
            "--------------------\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-10-1813425700.py\", line 2, in <cell line: 0>\n",
            "    api.jit(square_add_prim)(2., 10.)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/jax/_src/traceback_util.py\", line 180, in reraise_with_filtered_traceback\n",
            "    return fun(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/jax/_src/pjit.py\", line 341, in cache_miss\n",
            "    pgle_profiler) = _python_pjit_helper(fun, jit_info, *args, **kwargs)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "NotImplementedError: MLIR translation rule for primitive 'multiply_add' not found for platform cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### XLA Compilation rules"
      ],
      "metadata": {
        "id": "Qs3nJfQwtksp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "JAX compilation works by compiling each primitive into a graph of XLA operations.\n",
        "\n",
        "This is the biggest hurdle to adding new functionality to JAX, because the set of XLA operations is limited, and JAX already has pre-defined primitives for most of them. However, XLA includes a CustomCall operation that can be used to encapsulate arbitrary functionality defined using C++."
      ],
      "metadata": {
        "id": "NVwFd1GCtm7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from jax._src.lib.mlir.dialects import hlo\n",
        "\n",
        "@trace(\"multiply_add_lowering\")\n",
        "def multiply_add_lowering(ctx, xc, yc, zc):\n",
        "  \"\"\"The compilation to XLA of the primitive.\n",
        "\n",
        "  Given an mlir.ir.Value for each argument, return the mlir.ir.Values for\n",
        "  the results of the function.\n",
        "\n",
        "  Does not need to be a JAX-traceable function.\n",
        "  \"\"\"\n",
        "  return [hlo.AddOp(hlo.MulOp(xc, yc), zc).result]\n",
        "\n",
        "# Now, register the lowering rule with JAX.\n",
        "# For GPU, refer to the https://docs.jax.dev/en/latest/Custom_Operation_for_GPUs.html\n",
        "from jax.interpreters import mlir\n",
        "\n",
        "mlir.register_lowering(multiply_add_p, multiply_add_lowering, platform='cpu')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "id": "5775sW0ctoFB",
        "outputId": "43bef8f9-e3b0-42c1-8477-8ccb60f857a0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function __main__.multiply_add_lowering(ctx, xc, yc, zc)>"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>multiply_add_lowering</b><br/>def multiply_add_lowering(ctx, xc, yc, zc)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/tmp/ipython-input-11-563990782.py</a>The compilation to XLA of the primitive.\n",
              "\n",
              "Given an mlir.ir.Value for each argument, return the mlir.ir.Values for\n",
              "the results of the function.\n",
              "\n",
              "Does not need to be a JAX-traceable function.</pre></div>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will now succeed to apply jax.jit. Notice below that JAX first evaluates the function abstractly, which triggers the multiply_add_abstract_eval function, and then compiles the set of primitives it has encountered, including multiply_add. At this point JAX invokes multiply_add_lowering."
      ],
      "metadata": {
        "id": "9OHW7cCCucs5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assert api.jit(lambda x, y: square_add_prim(x, y))(2., 10.) == 14."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gFKh-70uf2y",
        "outputId": "9f26bc3e-2ae3-4a28-bd10-b92f93270a33"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "call square_add_prim(Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace>, Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace>)\n",
            "  call multiply_add_prim(Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace>, Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace>, Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace>)\n",
            "    call multiply_add_abstract_eval(ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True))\n",
            "    |<- multiply_add_abstract_eval = ShapedArray(float32[])\n",
            "  |<- multiply_add_prim = Traced<ShapedArray(float32[])>with<DynamicJaxprTrace>\n",
            "|<- square_add_prim = Traced<ShapedArray(float32[])>with<DynamicJaxprTrace>\n",
            "call multiply_add_lowering(LoweringRuleContext(module_context=ModuleContext(context=<jax._src.interpreters.mlir.JaxIrContext object at 0x781170fea840>, module=<jaxlib.mlir._mlir_libs._mlir.ir.Module object at 0x781170f81b20>, ip=<jaxlib.mlir._mlir_libs._mlir.ir.InsertionPoint object at 0x781170f81390>, symbol_table=<jaxlib.mlir._mlir_libs._mlir.ir.SymbolTable object at 0x78117100f840>, platforms=('cpu',), backend=<jaxlib.xla_extension.Client object at 0x7811712dae90>, axis_context=ShardingContext(num_devices=1, device_assignment=None, abstract_mesh=None), keepalives=[], channel_iterator=count(1), host_callbacks=[], shape_poly_state=<jax._src.interpreters.mlir.ShapePolyLoweringState object at 0x7811706fb090>, all_default_mem_kind=True, cached_primitive_lowerings={}, traceback_caches=TracebackCaches(traceback_cache={<jaxlib.xla_extension.Traceback object at 0x2d60f040>: loc(callsite(\"multiply_add_prim\"(\"/tmp/ipython-input-4-2637569133.py\":12:9 to :37) at callsite(\"trace.<locals>.trace_func.<locals>.func_wrapper\"(\"/tmp/ipython-input-2-2803309189.py\":48:12 to :23) at callsite(\"square_add_prim\"(\"/tmp/ipython-input-4-2637569133.py\":17:9 to :35) at callsite(\"trace.<locals>.trace_func.<locals>.func_wrapper\"(\"/tmp/ipython-input-2-2803309189.py\":48:12 to :23) at callsite(\"<lambda>\"(\"/tmp/ipython-input-12-1570919344.py\":1:28 to :49) at callsite(\"<module>\"(\"/tmp/ipython-input-12-1570919344.py\":1:7 to :59) at callsite(\"InteractiveShell.run_code\"(\"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\":3553:20 to :69) at callsite(\"InteractiveShell.run_ast_nodes\"(\"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\":3473:24 to :70) at callsite(\"InteractiveShell.run_cell_async\"(\"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\":3257:29 to 3258:85) at \"_pseudo_sync_runner\"(\"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\":78:8 to :23)))))))))))}, location_cache={(<code object multiply_add_prim at 0x781170701b30, file \"/tmp/ipython-input-4-2637569133.py\", line 5>, 54): loc(\"multiply_add_prim\"(\"/tmp/ipython-input-4-2637569133.py\":12:9 to :37)), (<code object func_wrapper at 0x781188cb96b0, file \"/tmp/ipython-input-2-2803309189.py\", line 45>, 98): loc(\"trace.<locals>.trace_func.<locals>.func_wrapper\"(\"/tmp/ipython-input-2-2803309189.py\":48:12 to :23)), (<code object square_add_prim at 0x781170455290, file \"/tmp/ipython-input-4-2637569133.py\", line 14>, 32): loc(\"square_add_prim\"(\"/tmp/ipython-input-4-2637569133.py\":17:9 to :35)), (<code object <lambda> at 0x781170707ad0, file \"/tmp/ipython-input-12-1570919344.py\", line 1>, 30): loc(\"<lambda>\"(\"/tmp/ipython-input-12-1570919344.py\":1:28 to :49)), (<code object <cell line: 0> at 0x781170469680, file \"/tmp/ipython-input-12-1570919344.py\", line 1>, 54): loc(\"<module>\"(\"/tmp/ipython-input-12-1570919344.py\":1:7 to :59)), (<code object run_code at 0x2ac2a720, file \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3512>, 426): loc(\"InteractiveShell.run_code\"(\"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\":3553:20 to :69)), (<code object run_ast_nodes at 0x2ac29a30, file \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3360>, 1710): loc(\"InteractiveShell.run_ast_nodes\"(\"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\":3473:24 to :70)), (<code object run_cell_async at 0x2ac2d1c0, file \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3084>, 2004): loc(\"InteractiveShell.run_cell_async\"(\"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\":3257:29 to 3258:85)), (<code object _pseudo_sync_runner at 0x781194b9c930, file \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 68>, 30): loc(\"_pseudo_sync_runner\"(\"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\":78:8 to :23))}, canonical_name_cache={'/tmp/ipython-input-4-2637569133.py': '/tmp/ipython-input-4-2637569133.py', '/tmp/ipython-input-2-2803309189.py': '/tmp/ipython-input-2-2803309189.py', '/tmp/ipython-input-12-1570919344.py': '/tmp/ipython-input-12-1570919344.py', '/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py': '/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py', '/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py': '/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py'}, is_user_file_cache={'/usr/local/lib/python3.11/dist-packages/jax/_src/source_info_util.py': False, '/usr/local/lib/python3.11/dist-packages/jax/_src/interpreters/partial_eval.py': False, '/usr/local/lib/python3.11/dist-packages/jax/_src/core.py': False, '/tmp/ipython-input-4-2637569133.py': True, '/tmp/ipython-input-2-2803309189.py': True, '/tmp/ipython-input-12-1570919344.py': True, '/usr/local/lib/python3.11/dist-packages/jax/_src/linear_util.py': False, '/usr/local/lib/python3.11/dist-packages/jax/_src/api_util.py': False, '/usr/local/lib/python3.11/dist-packages/jax/_src/profiler.py': False, '/usr/local/lib/python3.11/dist-packages/jax/_src/pjit.py': False, '/usr/local/lib/python3.11/dist-packages/jax/_src/traceback_util.py': False, '/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py': True, '/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py': True}), lowering_parameters=LoweringParameters(override_lowering_rules=None, global_constant_computation=False, for_export=False, export_ignore_forward_compatibility=False)), name_stack=NameStack(stack=(Scope(name='jit(<lambda>)'), Scope(name='jit(main)'))), primitive=multiply_add, avals_in=[ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True)], avals_out=[ShapedArray(float32[])], tokens_in=<jax._src.interpreters.mlir.TokenSet object at 0x781170f59f90>, tokens_out=None, axis_size_env=None, dim_var_values=[], jaxpr_eqn_ctx=JaxprEqnContext(compute_type=None, threefry_partitionable=True, cur_abstract_mesh=AbstractMesh((), axis_types={}), xla_metadata=None), platforms=None), Value(<block argument> of type 'tensor<f32>' at index: 0), Value(<block argument> of type 'tensor<f32>' at index: 0), Value(<block argument> of type 'tensor<f32>' at index: 1))\n",
            "|<- multiply_add_lowering = [<jaxlib.mlir._mlir_libs._mlir.ir.OpResult object at 0x78117052a330>]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is another use of jit, where you compile only with respect to the first argument. Notice how the second argument to square_add_prim is concrete, which leads in the third argument to multiply_add_abstract_eval being ConcreteArray. Notice that multiply_add_abstract_eval may be used with both ShapedArray and ConcreteArray."
      ],
      "metadata": {
        "id": "D4Z_dhNEujUZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assert api.jit(lambda x, y: square_add_prim(x, y),\n",
        "               static_argnums=1)(2., 10.) == 14."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKP6thAKukwJ",
        "outputId": "22fc88fd-495c-43b2-a8d4-175835e06b19"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "call square_add_prim(Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace>, 10.0)\n",
            "  call multiply_add_prim(Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace>, Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace>, 10.0)\n",
            "    call multiply_add_abstract_eval(ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True))\n",
            "    |<- multiply_add_abstract_eval = ShapedArray(float32[])\n",
            "  |<- multiply_add_prim = Traced<ShapedArray(float32[])>with<DynamicJaxprTrace>\n",
            "|<- square_add_prim = Traced<ShapedArray(float32[])>with<DynamicJaxprTrace>\n",
            "call multiply_add_lowering(LoweringRuleContext(module_context=ModuleContext(context=<jax._src.interpreters.mlir.JaxIrContext object at 0x781170febc80>, module=<jaxlib.mlir._mlir_libs._mlir.ir.Module object at 0x781170f8c540>, ip=<jaxlib.mlir._mlir_libs._mlir.ir.InsertionPoint object at 0x781170f8c0d0>, symbol_table=<jaxlib.mlir._mlir_libs._mlir.ir.SymbolTable object at 0x781170fe1c20>, platforms=('cpu',), backend=<jaxlib.xla_extension.Client object at 0x7811712dae90>, axis_context=ShardingContext(num_devices=1, device_assignment=None, abstract_mesh=None), keepalives=[], channel_iterator=count(1), host_callbacks=[], shape_poly_state=<jax._src.interpreters.mlir.ShapePolyLoweringState object at 0x781170721250>, all_default_mem_kind=True, cached_primitive_lowerings={}, traceback_caches=TracebackCaches(traceback_cache={<jaxlib.xla_extension.Traceback object at 0x2d68a6e0>: loc(callsite(\"multiply_add_prim\"(\"/tmp/ipython-input-4-2637569133.py\":12:9 to :37) at callsite(\"trace.<locals>.trace_func.<locals>.func_wrapper\"(\"/tmp/ipython-input-2-2803309189.py\":48:12 to :23) at callsite(\"square_add_prim\"(\"/tmp/ipython-input-4-2637569133.py\":17:9 to :35) at callsite(\"trace.<locals>.trace_func.<locals>.func_wrapper\"(\"/tmp/ipython-input-2-2803309189.py\":48:12 to :23) at callsite(\"<lambda>\"(\"/tmp/ipython-input-13-4165789807.py\":1:28 to :49) at callsite(\"<module>\"(\"/tmp/ipython-input-13-4165789807.py\":1:7 to 2:41) at callsite(\"InteractiveShell.run_code\"(\"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\":3553:20 to :69) at callsite(\"InteractiveShell.run_ast_nodes\"(\"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\":3473:24 to :70) at callsite(\"InteractiveShell.run_cell_async\"(\"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\":3257:29 to 3258:85) at \"_pseudo_sync_runner\"(\"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\":78:8 to :23)))))))))))}, location_cache={(<code object multiply_add_prim at 0x781170701b30, file \"/tmp/ipython-input-4-2637569133.py\", line 5>, 54): loc(\"multiply_add_prim\"(\"/tmp/ipython-input-4-2637569133.py\":12:9 to :37)), (<code object func_wrapper at 0x781188cb96b0, file \"/tmp/ipython-input-2-2803309189.py\", line 45>, 98): loc(\"trace.<locals>.trace_func.<locals>.func_wrapper\"(\"/tmp/ipython-input-2-2803309189.py\":48:12 to :23)), (<code object square_add_prim at 0x781170455290, file \"/tmp/ipython-input-4-2637569133.py\", line 14>, 32): loc(\"square_add_prim\"(\"/tmp/ipython-input-4-2637569133.py\":17:9 to :35)), (<code object <lambda> at 0x781170455370, file \"/tmp/ipython-input-13-4165789807.py\", line 1>, 30): loc(\"<lambda>\"(\"/tmp/ipython-input-13-4165789807.py\":1:28 to :49)), (<code object <cell line: 0> at 0x781170469bd0, file \"/tmp/ipython-input-13-4165789807.py\", line 1>, 58): loc(\"<module>\"(\"/tmp/ipython-input-13-4165789807.py\":1:7 to 2:41)), (<code object run_code at 0x2ac2a720, file \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3512>, 426): loc(\"InteractiveShell.run_code\"(\"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\":3553:20 to :69)), (<code object run_ast_nodes at 0x2ac29a30, file \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3360>, 1710): loc(\"InteractiveShell.run_ast_nodes\"(\"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\":3473:24 to :70)), (<code object run_cell_async at 0x2ac2d1c0, file \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3084>, 2004): loc(\"InteractiveShell.run_cell_async\"(\"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\":3257:29 to 3258:85)), (<code object _pseudo_sync_runner at 0x781194b9c930, file \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 68>, 30): loc(\"_pseudo_sync_runner\"(\"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\":78:8 to :23))}, canonical_name_cache={'/tmp/ipython-input-4-2637569133.py': '/tmp/ipython-input-4-2637569133.py', '/tmp/ipython-input-2-2803309189.py': '/tmp/ipython-input-2-2803309189.py', '/tmp/ipython-input-13-4165789807.py': '/tmp/ipython-input-13-4165789807.py', '/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py': '/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py', '/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py': '/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py'}, is_user_file_cache={'/usr/local/lib/python3.11/dist-packages/jax/_src/source_info_util.py': False, '/usr/local/lib/python3.11/dist-packages/jax/_src/interpreters/partial_eval.py': False, '/usr/local/lib/python3.11/dist-packages/jax/_src/core.py': False, '/tmp/ipython-input-4-2637569133.py': True, '/tmp/ipython-input-2-2803309189.py': True, '/tmp/ipython-input-13-4165789807.py': True, '/usr/local/lib/python3.11/dist-packages/jax/_src/linear_util.py': False, '/usr/local/lib/python3.11/dist-packages/jax/_src/api_util.py': False, '/usr/local/lib/python3.11/dist-packages/jax/_src/profiler.py': False, '/usr/local/lib/python3.11/dist-packages/jax/_src/pjit.py': False, '/usr/local/lib/python3.11/dist-packages/jax/_src/traceback_util.py': False, '/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py': True, '/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py': True}), lowering_parameters=LoweringParameters(override_lowering_rules=None, global_constant_computation=False, for_export=False, export_ignore_forward_compatibility=False)), name_stack=NameStack(stack=(Scope(name='jit(<lambda>)'), Scope(name='jit(main)'))), primitive=multiply_add, avals_in=[ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True)], avals_out=[ShapedArray(float32[])], tokens_in=<jax._src.interpreters.mlir.TokenSet object at 0x781170f76950>, tokens_out=None, axis_size_env=None, dim_var_values=[], jaxpr_eqn_ctx=JaxprEqnContext(compute_type=None, threefry_partitionable=True, cur_abstract_mesh=AbstractMesh((), axis_types={}), xla_metadata=None), platforms=None), Value(<block argument> of type 'tensor<f32>' at index: 0), Value(<block argument> of type 'tensor<f32>' at index: 0), Value(%0 = \"stablehlo.constant\"() <{value = dense<1.000000e+01> : tensor<f32>}> : () -> tensor<f32>))\n",
            "|<- multiply_add_lowering = [<jaxlib.mlir._mlir_libs._mlir.ir.OpResult object at 0x7811706fb530>]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Forward differentiation"
      ],
      "metadata": {
        "id": "WrW7qLyyu2Gw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "JAX implements forward differentiation in the form of a Jacobian-Vector Product (JVP) (you can learn more about it in Advanced automatic differentiation).\n",
        "\n",
        "If you attempt to compute the jvp function, you’ll get an error because you have not yet told JAX how to differentiate the multiply_add primitive."
      ],
      "metadata": {
        "id": "8wr8Zd6-u4Jg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The second argument is set to `(2., 10.)` values where you\n",
        "# evaluate the Jacobian, and the third argument `(1., 1.)`\n",
        "# contains the values of the tangents for the arguments.\n",
        "with expectNotImplementedError():\n",
        "  api.jvp(square_add_prim, (2., 10.), (1., 1.))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAa43KCQu5VJ",
        "outputId": "0684acdc-cd28-43fb-f038-5198170fecc9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "call square_add_prim(Traced<ShapedArray(float32[], weak_type=True)>, Traced<ShapedArray(float32[], weak_type=True)>)\n",
            "  call multiply_add_prim(Traced<ShapedArray(float32[], weak_type=True)>, Traced<ShapedArray(float32[], weak_type=True)>, Traced<ShapedArray(float32[], weak_type=True)>)\n",
            "\n",
            "Found expected exception:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-14-459539105.py\", line 5, in <cell line: 0>\n",
            "    api.jvp(square_add_prim, (2., 10.), (1., 1.))\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/jax/_src/traceback_util.py\", line 180, in reraise_with_filtered_traceback\n",
            "    return fun(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/jax/_src/api.py\", line 1739, in jvp\n",
            "    return _jvp(lu.wrap_init(fun, debug_info=debug_info(\"jvp\", fun, primals, {})),\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "NotImplementedError: Differentiation rule for 'multiply_add' not implemented\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from jax.interpreters import ad\n",
        "\n",
        "@trace(\"multiply_add_value_and_jvp\")\n",
        "def multiply_add_value_and_jvp(arg_values, arg_tangents):\n",
        "  \"\"\"Evaluates the primal output and the tangents (Jacobian-vector product).\n",
        "\n",
        "  Given values of the arguments and perturbation of the arguments (tangents),\n",
        "  compute the output of the primitive and the perturbation of the output.\n",
        "\n",
        "  This method must be JAX-traceable. JAX may invoke it with abstract values\n",
        "  for the arguments and tangents.\n",
        "\n",
        "  Args:\n",
        "    arg_values: A tuple of arguments\n",
        "    arg_tangents: A tuple with the tangents of the arguments. The tuple has\n",
        "      the same length as the arg_values. Some of the tangents may also be the\n",
        "      special value `ad.Zero` to specify a zero tangent\n",
        "\n",
        "  Returns:\n",
        "     A pair of the primal output and the tangent.\n",
        "  \"\"\"\n",
        "  x, y, z = arg_values\n",
        "  xt, yt, zt = arg_tangents\n",
        "  _trace(\"Primal evaluation:\")\n",
        "  # Now, you have a JAX-traceable computation of the output.\n",
        "  # Normally, you can use the multiply add (`ma`) primitive itself to compute the primal output.\n",
        "  primal_out = multiply_add_prim(x, y, z)\n",
        "\n",
        "  _trace(\"Tangent evaluation:\")\n",
        "  # You must use a JAX-traceable way to compute the tangent. It turns out that\n",
        "  # the output tangent can be computed as (xt * y + x * yt + zt),\n",
        "  # which you can implement in a JAX-traceable way using the same \"multiply_add_prim\" primitive.\n",
        "\n",
        "  # You do need to deal specially with `Zero`. Here, you just turn it into a\n",
        "  # proper tensor of 0s (of the same shape as 'x').\n",
        "  # An alternative would be to check for `Zero` and perform algebraic\n",
        "  # simplification of the output tangent computation.\n",
        "  def make_zero(tan):\n",
        "    return lax.zeros_like_array(x) if type(tan) is ad.Zero else tan\n",
        "\n",
        "  output_tangent = multiply_add_prim(make_zero(xt), y, multiply_add_prim(x, make_zero(yt), make_zero(zt)))\n",
        "  return (primal_out, output_tangent)\n",
        "\n",
        "# Register the forward differentiation rule with JAX:\n",
        "ad.primitive_jvps[multiply_add_p] = multiply_add_value_and_jvp"
      ],
      "metadata": {
        "id": "hEHN3A5fvI_p"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tangent is: xt*y + x*yt + zt = 1.*2. + 2.*1. + 1. = 5.\n",
        "assert api.jvp(square_add_prim, (2., 10.), (1., 1.)) == (14., 5.)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o74fSRhyvLxp",
        "outputId": "05c7cab7-a835-4377-9ca1-29b8d7c50264"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "call square_add_prim(Traced<ShapedArray(float32[], weak_type=True)>, Traced<ShapedArray(float32[], weak_type=True)>)\n",
            "  call multiply_add_prim(Traced<ShapedArray(float32[], weak_type=True)>, Traced<ShapedArray(float32[], weak_type=True)>, Traced<ShapedArray(float32[], weak_type=True)>)\n",
            "    call multiply_add_value_and_jvp((2.0, 2.0, 10.0), (1.0, 1.0, 1.0))\n",
            "      Primal evaluation:\n",
            "      call multiply_add_prim(2.0, 2.0, 10.0)\n",
            "        call multiply_add_impl(2.0, 2.0, 10.0)\n",
            "        |<- multiply_add_impl = 14.0\n",
            "      |<- multiply_add_prim = 14.0\n",
            "      Tangent evaluation:\n",
            "      call multiply_add_prim(2.0, 1.0, 1.0)\n",
            "        call multiply_add_impl(2.0, 1.0, 1.0)\n",
            "        |<- multiply_add_impl = 3.0\n",
            "      |<- multiply_add_prim = 3.0\n",
            "      call multiply_add_prim(1.0, 2.0, 3.0)\n",
            "        call multiply_add_impl(1.0, 2.0, 3.0)\n",
            "        |<- multiply_add_impl = 5.0\n",
            "      |<- multiply_add_prim = 5.0\n",
            "    |<- multiply_add_value_and_jvp = (14.0, 5.0)\n",
            "  |<- multiply_add_prim = Traced<ShapedArray(float32[])>\n",
            "|<- square_add_prim = Traced<ShapedArray(float32[])>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### JIT of forward differentiation"
      ],
      "metadata": {
        "id": "txDxsq7cvSsh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can apply jit to the forward differentiation function:"
      ],
      "metadata": {
        "id": "Dr6sqpzQwB8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assert api.jit(lambda arg_values, arg_tangents:\n",
        "                   api.jvp(square_add_prim, arg_values, arg_tangents))(\n",
        "         (2., 10.), (1., 1.)) == (14., 5.)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvBThXX6wC6B",
        "outputId": "fe9cbb55-917b-4ac8-a02c-c2306406cc01"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "call square_add_prim(Traced<ShapedArray(float32[], weak_type=True)>, Traced<ShapedArray(float32[], weak_type=True)>)\n",
            "  call multiply_add_prim(Traced<ShapedArray(float32[], weak_type=True)>, Traced<ShapedArray(float32[], weak_type=True)>, Traced<ShapedArray(float32[], weak_type=True)>)\n",
            "    call multiply_add_value_and_jvp((Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace>, Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace>, Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace>), (Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace>, Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace>, Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace>))\n",
            "      Primal evaluation:\n",
            "      call multiply_add_prim(Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace>, Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace>, Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace>)\n",
            "        call multiply_add_abstract_eval(ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True))\n",
            "        |<- multiply_add_abstract_eval = ShapedArray(float32[])\n",
            "      |<- multiply_add_prim = Traced<ShapedArray(float32[])>with<DynamicJaxprTrace>\n",
            "      Tangent evaluation:\n",
            "      call multiply_add_prim(Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace>, Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace>, Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace>)\n",
            "        call multiply_add_abstract_eval(ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True))\n",
            "        |<- multiply_add_abstract_eval = ShapedArray(float32[])\n",
            "      |<- multiply_add_prim = Traced<ShapedArray(float32[])>with<DynamicJaxprTrace>\n",
            "      call multiply_add_prim(Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace>, Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace>, Traced<ShapedArray(float32[])>with<DynamicJaxprTrace>)\n",
            "        call multiply_add_abstract_eval(ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True), ShapedArray(float32[]))\n",
            "        |<- multiply_add_abstract_eval = ShapedArray(float32[])\n",
            "      |<- multiply_add_prim = Traced<ShapedArray(float32[])>with<DynamicJaxprTrace>\n",
            "    |<- multiply_add_value_and_jvp = (Traced<ShapedArray(float32[])>with<DynamicJaxprTrace>, Traced<ShapedArray(float32[])>with<DynamicJaxprTrace>)\n",
            "  |<- multiply_add_prim = Traced<ShapedArray(float32[])>\n",
            "|<- square_add_prim = Traced<ShapedArray(float32[])>\n",
            "call multiply_add_lowering(LoweringRuleContext(module_context=ModuleContext(context=<jax._src.interpreters.mlir.JaxIrContext object at 0x7811710200e0>, module=<jaxlib.mlir._mlir_libs._mlir.ir.Module object at 0x781170f8dc10>, ip=<jaxlib.mlir._mlir_libs._mlir.ir.InsertionPoint object at 0x781170f94a30>, symbol_table=<jaxlib.mlir._mlir_libs._mlir.ir.SymbolTable object at 0x781170f4c480>, platforms=('cpu',), backend=<jaxlib.xla_extension.Client object at 0x7811712dae90>, axis_context=ShardingContext(num_devices=1, device_assignment=None, abstract_mesh=None), keepalives=[], channel_iterator=count(1), host_callbacks=[], shape_poly_state=<jax._src.interpreters.mlir.ShapePolyLoweringState object at 0x781170f92990>, all_default_mem_kind=True, cached_primitive_lowerings={}, traceback_caches=TracebackCaches(traceback_cache={<jaxlib.xla_extension.Traceback object at 0x2d8fc7c0>: loc(callsite(\"multiply_add_prim\"(\"/tmp/ipython-input-4-2637569133.py\":12:9 to :37) at callsite(\"trace.<locals>.trace_func.<locals>.func_wrapper\"(\"/tmp/ipython-input-2-2803309189.py\":48:12 to :23) at callsite(\"multiply_add_value_and_jvp\"(\"/tmp/ipython-input-16-347789876.py\":27:15 to :41) at callsite(\"trace.<locals>.trace_func.<locals>.func_wrapper\"(\"/tmp/ipython-input-2-2803309189.py\":48:12 to :23) at callsite(\"multiply_add_prim\"(\"/tmp/ipython-input-4-2637569133.py\":12:9 to :37) at callsite(\"trace.<locals>.trace_func.<locals>.func_wrapper\"(\"/tmp/ipython-input-2-2803309189.py\":48:12 to :23) at callsite(\"square_add_prim\"(\"/tmp/ipython-input-4-2637569133.py\":17:9 to :35) at callsite(\"trace.<locals>.trace_func.<locals>.func_wrapper\"(\"/tmp/ipython-input-2-2803309189.py\":48:12 to :23) at callsite(\"<lambda>\"(\"/tmp/ipython-input-18-2145028508.py\":2:19 to :69) at \"<module>\"(\"/tmp/ipython-input-18-2145028508.py\":1:7 to 3:29)))))))))))}, location_cache={(<code object multiply_add_prim at 0x781170701b30, file \"/tmp/ipython-input-4-2637569133.py\", line 5>, 54): loc(\"multiply_add_prim\"(\"/tmp/ipython-input-4-2637569133.py\":12:9 to :37)), (<code object func_wrapper at 0x781188cb96b0, file \"/tmp/ipython-input-2-2803309189.py\", line 45>, 98): loc(\"trace.<locals>.trace_func.<locals>.func_wrapper\"(\"/tmp/ipython-input-2-2803309189.py\":48:12 to :23)), (<code object multiply_add_value_and_jvp at 0x7811706fc570, file \"/tmp/ipython-input-16-347789876.py\", line 3>, 88): loc(\"multiply_add_value_and_jvp\"(\"/tmp/ipython-input-16-347789876.py\":27:15 to :41)), (<code object square_add_prim at 0x781170455290, file \"/tmp/ipython-input-4-2637569133.py\", line 14>, 32): loc(\"square_add_prim\"(\"/tmp/ipython-input-4-2637569133.py\":17:9 to :35)), (<code object <lambda> at 0x781170701130, file \"/tmp/ipython-input-18-2145028508.py\", line 1>, 64): loc(\"<lambda>\"(\"/tmp/ipython-input-18-2145028508.py\":2:19 to :69)), (<code object <cell line: 0> at 0x781170469ce0, file \"/tmp/ipython-input-18-2145028508.py\", line 1>, 54): loc(\"<module>\"(\"/tmp/ipython-input-18-2145028508.py\":1:7 to 3:29))}, canonical_name_cache={'/tmp/ipython-input-4-2637569133.py': '/tmp/ipython-input-4-2637569133.py', '/tmp/ipython-input-2-2803309189.py': '/tmp/ipython-input-2-2803309189.py', '/tmp/ipython-input-16-347789876.py': '/tmp/ipython-input-16-347789876.py', '/tmp/ipython-input-18-2145028508.py': '/tmp/ipython-input-18-2145028508.py'}, is_user_file_cache={'/usr/local/lib/python3.11/dist-packages/jax/_src/source_info_util.py': False, '/usr/local/lib/python3.11/dist-packages/jax/_src/interpreters/partial_eval.py': False, '/usr/local/lib/python3.11/dist-packages/jax/_src/core.py': False, '/tmp/ipython-input-4-2637569133.py': True, '/tmp/ipython-input-2-2803309189.py': True, '/tmp/ipython-input-16-347789876.py': True, '/usr/local/lib/python3.11/dist-packages/jax/_src/interpreters/ad.py': False, '/usr/local/lib/python3.11/dist-packages/jax/_src/linear_util.py': False, '/usr/local/lib/python3.11/dist-packages/jax/_src/api_util.py': False, '/usr/local/lib/python3.11/dist-packages/jax/_src/api.py': False, '/usr/local/lib/python3.11/dist-packages/jax/_src/traceback_util.py': False, '/tmp/ipython-input-18-2145028508.py': True, '/usr/local/lib/python3.11/dist-packages/jax/_src/profiler.py': False, '/usr/local/lib/python3.11/dist-packages/jax/_src/pjit.py': False}), lowering_parameters=LoweringParameters(override_lowering_rules=None, global_constant_computation=False, for_export=False, export_ignore_forward_compatibility=False)), name_stack=NameStack(stack=(Scope(name='jit(<lambda>)'), Scope(name='jit(main)'), Transform(name='jvp'))), primitive=multiply_add, avals_in=[ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True)], avals_out=[ShapedArray(float32[])], tokens_in=<jax._src.interpreters.mlir.TokenSet object at 0x781170fd4950>, tokens_out=None, axis_size_env=None, dim_var_values=[], jaxpr_eqn_ctx=JaxprEqnContext(compute_type=None, threefry_partitionable=True, cur_abstract_mesh=AbstractMesh((), axis_types={}), xla_metadata=None), platforms=None), Value(<block argument> of type 'tensor<f32>' at index: 0), Value(<block argument> of type 'tensor<f32>' at index: 0), Value(<block argument> of type 'tensor<f32>' at index: 1))\n",
            "|<- multiply_add_lowering = [<jaxlib.mlir._mlir_libs._mlir.ir.OpResult object at 0x7811710fc030>]\n",
            "call multiply_add_lowering(LoweringRuleContext(module_context=ModuleContext(context=<jax._src.interpreters.mlir.JaxIrContext object at 0x7811710200e0>, module=<jaxlib.mlir._mlir_libs._mlir.ir.Module object at 0x781170f8dc10>, ip=<jaxlib.mlir._mlir_libs._mlir.ir.InsertionPoint object at 0x781170f94a30>, symbol_table=<jaxlib.mlir._mlir_libs._mlir.ir.SymbolTable object at 0x781170f4c480>, platforms=('cpu',), backend=<jaxlib.xla_extension.Client object at 0x7811712dae90>, axis_context=ShardingContext(num_devices=1, device_assignment=None, abstract_mesh=None), keepalives=[], channel_iterator=count(1), host_callbacks=[], shape_poly_state=<jax._src.interpreters.mlir.ShapePolyLoweringState object at 0x781170f92990>, all_default_mem_kind=True, cached_primitive_lowerings={}, traceback_caches=TracebackCaches(traceback_cache={<jaxlib.xla_extension.Traceback object at 0x2d8fc7c0>: loc(callsite(\"multiply_add_prim\"(\"/tmp/ipython-input-4-2637569133.py\":12:9 to :37) at callsite(\"trace.<locals>.trace_func.<locals>.func_wrapper\"(\"/tmp/ipython-input-2-2803309189.py\":48:12 to :23) at callsite(\"multiply_add_value_and_jvp\"(\"/tmp/ipython-input-16-347789876.py\":27:15 to :41) at callsite(\"trace.<locals>.trace_func.<locals>.func_wrapper\"(\"/tmp/ipython-input-2-2803309189.py\":48:12 to :23) at callsite(\"multiply_add_prim\"(\"/tmp/ipython-input-4-2637569133.py\":12:9 to :37) at callsite(\"trace.<locals>.trace_func.<locals>.func_wrapper\"(\"/tmp/ipython-input-2-2803309189.py\":48:12 to :23) at callsite(\"square_add_prim\"(\"/tmp/ipython-input-4-2637569133.py\":17:9 to :35) at callsite(\"trace.<locals>.trace_func.<locals>.func_wrapper\"(\"/tmp/ipython-input-2-2803309189.py\":48:12 to :23) at callsite(\"<lambda>\"(\"/tmp/ipython-input-18-2145028508.py\":2:19 to :69) at \"<module>\"(\"/tmp/ipython-input-18-2145028508.py\":1:7 to 3:29))))))))))), <jaxlib.xla_extension.Traceback object at 0x2d589930>: loc(callsite(\"multiply_add_prim\"(\"/tmp/ipython-input-4-2637569133.py\":12:9 to :37) at callsite(\"trace.<locals>.trace_func.<locals>.func_wrapper\"(\"/tmp/ipython-input-2-2803309189.py\":48:12 to :23) at callsite(\"multiply_add_value_and_jvp\"(\"/tmp/ipython-input-16-347789876.py\":41:55 to :105) at callsite(\"trace.<locals>.trace_func.<locals>.func_wrapper\"(\"/tmp/ipython-input-2-2803309189.py\":48:12 to :23) at callsite(\"multiply_add_prim\"(\"/tmp/ipython-input-4-2637569133.py\":12:9 to :37) at callsite(\"trace.<locals>.trace_func.<locals>.func_wrapper\"(\"/tmp/ipython-input-2-2803309189.py\":48:12 to :23) at callsite(\"square_add_prim\"(\"/tmp/ipython-input-4-2637569133.py\":17:9 to :35) at callsite(\"trace.<locals>.trace_func.<locals>.func_wrapper\"(\"/tmp/ipython-input-2-2803309189.py\":48:12 to :23) at callsite(\"<lambda>\"(\"/tmp/ipython-input-18-2145028508.py\":2:19 to :69) at \"<module>\"(\"/tmp/ipython-input-18-2145028508.py\":1:7 to 3:29)))))))))))}, location_cache={(<code object multiply_add_prim at 0x781170701b30, file \"/tmp/ipython-input-4-2637569133.py\", line 5>, 54): loc(\"multiply_add_prim\"(\"/tmp/ipython-input-4-2637569133.py\":12:9 to :37)), (<code object func_wrapper at 0x781188cb96b0, file \"/tmp/ipython-input-2-2803309189.py\", line 45>, 98): loc(\"trace.<locals>.trace_func.<locals>.func_wrapper\"(\"/tmp/ipython-input-2-2803309189.py\":48:12 to :23)), (<code object multiply_add_value_and_jvp at 0x7811706fc570, file \"/tmp/ipython-input-16-347789876.py\", line 3>, 88): loc(\"multiply_add_value_and_jvp\"(\"/tmp/ipython-input-16-347789876.py\":27:15 to :41)), (<code object square_add_prim at 0x781170455290, file \"/tmp/ipython-input-4-2637569133.py\", line 14>, 32): loc(\"square_add_prim\"(\"/tmp/ipython-input-4-2637569133.py\":17:9 to :35)), (<code object <lambda> at 0x781170701130, file \"/tmp/ipython-input-18-2145028508.py\", line 1>, 64): loc(\"<lambda>\"(\"/tmp/ipython-input-18-2145028508.py\":2:19 to :69)), (<code object <cell line: 0> at 0x781170469ce0, file \"/tmp/ipython-input-18-2145028508.py\", line 1>, 54): loc(\"<module>\"(\"/tmp/ipython-input-18-2145028508.py\":1:7 to 3:29)), (<code object multiply_add_value_and_jvp at 0x7811706fc570, file \"/tmp/ipython-input-16-347789876.py\", line 3>, 232): loc(\"multiply_add_value_and_jvp\"(\"/tmp/ipython-input-16-347789876.py\":41:55 to :105))}, canonical_name_cache={'/tmp/ipython-input-4-2637569133.py': '/tmp/ipython-input-4-2637569133.py', '/tmp/ipython-input-2-2803309189.py': '/tmp/ipython-input-2-2803309189.py', '/tmp/ipython-input-16-347789876.py': '/tmp/ipython-input-16-347789876.py', '/tmp/ipython-input-18-2145028508.py': '/tmp/ipython-input-18-2145028508.py'}, is_user_file_cache={'/usr/local/lib/python3.11/dist-packages/jax/_src/source_info_util.py': False, '/usr/local/lib/python3.11/dist-packages/jax/_src/interpreters/partial_eval.py': False, '/usr/local/lib/python3.11/dist-packages/jax/_src/core.py': False, '/tmp/ipython-input-4-2637569133.py': True, '/tmp/ipython-input-2-2803309189.py': True, '/tmp/ipython-input-16-347789876.py': True, '/usr/local/lib/python3.11/dist-packages/jax/_src/interpreters/ad.py': False, '/usr/local/lib/python3.11/dist-packages/jax/_src/linear_util.py': False, '/usr/local/lib/python3.11/dist-packages/jax/_src/api_util.py': False, '/usr/local/lib/python3.11/dist-packages/jax/_src/api.py': False, '/usr/local/lib/python3.11/dist-packages/jax/_src/traceback_util.py': False, '/tmp/ipython-input-18-2145028508.py': True, '/usr/local/lib/python3.11/dist-packages/jax/_src/profiler.py': False, '/usr/local/lib/python3.11/dist-packages/jax/_src/pjit.py': False}), lowering_parameters=LoweringParameters(override_lowering_rules=None, global_constant_computation=False, for_export=False, export_ignore_forward_compatibility=False)), name_stack=NameStack(stack=(Scope(name='jit(<lambda>)'), Scope(name='jit(main)'), Transform(name='jvp'))), primitive=multiply_add, avals_in=[ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True)], avals_out=[ShapedArray(float32[])], tokens_in=<jax._src.interpreters.mlir.TokenSet object at 0x781170fd4750>, tokens_out=None, axis_size_env=None, dim_var_values=[], jaxpr_eqn_ctx=JaxprEqnContext(compute_type=None, threefry_partitionable=True, cur_abstract_mesh=AbstractMesh((), axis_types={}), xla_metadata=None), platforms=None), Value(<block argument> of type 'tensor<f32>' at index: 0), Value(<block argument> of type 'tensor<f32>' at index: 2), Value(<block argument> of type 'tensor<f32>' at index: 3))\n",
            "|<- multiply_add_lowering = [<jaxlib.mlir._mlir_libs._mlir.ir.OpResult object at 0x781170fd4ff0>]\n",
            "call multiply_add_lowering(LoweringRuleContext(module_context=ModuleContext(context=<jax._src.interpreters.mlir.JaxIrContext object at 0x7811710200e0>, module=<jaxlib.mlir._mlir_libs._mlir.ir.Module object at 0x781170f8dc10>, ip=<jaxlib.mlir._mlir_libs._mlir.ir.InsertionPoint object at 0x781170f94a30>, symbol_table=<jaxlib.mlir._mlir_libs._mlir.ir.SymbolTable object at 0x781170f4c480>, platforms=('cpu',), backend=<jaxlib.xla_extension.Client object at 0x7811712dae90>, axis_context=ShardingContext(num_devices=1, device_assignment=None, abstract_mesh=None), keepalives=[], channel_iterator=count(1), host_callbacks=[], shape_poly_state=<jax._src.interpreters.mlir.ShapePolyLoweringState object at 0x781170f92990>, all_default_mem_kind=True, cached_primitive_lowerings={}, traceback_caches=TracebackCaches(traceback_cache={<jaxlib.xla_extension.Traceback object at 0x2d8fc7c0>: loc(callsite(\"multiply_add_prim\"(\"/tmp/ipython-input-4-2637569133.py\":12:9 to :37) at callsite(\"trace.<locals>.trace_func.<locals>.func_wrapper\"(\"/tmp/ipython-input-2-2803309189.py\":48:12 to :23) at callsite(\"multiply_add_value_and_jvp\"(\"/tmp/ipython-input-16-347789876.py\":27:15 to :41) at callsite(\"trace.<locals>.trace_func.<locals>.func_wrapper\"(\"/tmp/ipython-input-2-2803309189.py\":48:12 to :23) at callsite(\"multiply_add_prim\"(\"/tmp/ipython-input-4-2637569133.py\":12:9 to :37) at callsite(\"trace.<locals>.trace_func.<locals>.func_wrapper\"(\"/tmp/ipython-input-2-2803309189.py\":48:12 to :23) at callsite(\"square_add_prim\"(\"/tmp/ipython-input-4-2637569133.py\":17:9 to :35) at callsite(\"trace.<locals>.trace_func.<locals>.func_wrapper\"(\"/tmp/ipython-input-2-2803309189.py\":48:12 to :23) at callsite(\"<lambda>\"(\"/tmp/ipython-input-18-2145028508.py\":2:19 to :69) at \"<module>\"(\"/tmp/ipython-input-18-2145028508.py\":1:7 to 3:29))))))))))), <jaxlib.xla_extension.Traceback object at 0x2d589930>: loc(callsite(\"multiply_add_prim\"(\"/tmp/ipython-input-4-2637569133.py\":12:9 to :37) at callsite(\"trace.<locals>.trace_func.<locals>.func_wrapper\"(\"/tmp/ipython-input-2-2803309189.py\":48:12 to :23) at callsite(\"multiply_add_value_and_jvp\"(\"/tmp/ipython-input-16-347789876.py\":41:55 to :105) at callsite(\"trace.<locals>.trace_func.<locals>.func_wrapper\"(\"/tmp/ipython-input-2-2803309189.py\":48:12 to :23) at callsite(\"multiply_add_prim\"(\"/tmp/ipython-input-4-2637569133.py\":12:9 to :37) at callsite(\"trace.<locals>.trace_func.<locals>.func_wrapper\"(\"/tmp/ipython-input-2-2803309189.py\":48:12 to :23) at callsite(\"square_add_prim\"(\"/tmp/ipython-input-4-2637569133.py\":17:9 to :35) at callsite(\"trace.<locals>.trace_func.<locals>.func_wrapper\"(\"/tmp/ipython-input-2-2803309189.py\":48:12 to :23) at callsite(\"<lambda>\"(\"/tmp/ipython-input-18-2145028508.py\":2:19 to :69) at \"<module>\"(\"/tmp/ipython-input-18-2145028508.py\":1:7 to 3:29))))))))))), <jaxlib.xla_extension.Traceback object at 0x2d7671a0>: loc(callsite(\"multiply_add_prim\"(\"/tmp/ipython-input-4-2637569133.py\":12:9 to :37) at callsite(\"trace.<locals>.trace_func.<locals>.func_wrapper\"(\"/tmp/ipython-input-2-2803309189.py\":48:12 to :23) at callsite(\"multiply_add_value_and_jvp\"(\"/tmp/ipython-input-16-347789876.py\":41:19 to :106) at callsite(\"trace.<locals>.trace_func.<locals>.func_wrapper\"(\"/tmp/ipython-input-2-2803309189.py\":48:12 to :23) at callsite(\"multiply_add_prim\"(\"/tmp/ipython-input-4-2637569133.py\":12:9 to :37) at callsite(\"trace.<locals>.trace_func.<locals>.func_wrapper\"(\"/tmp/ipython-input-2-2803309189.py\":48:12 to :23) at callsite(\"square_add_prim\"(\"/tmp/ipython-input-4-2637569133.py\":17:9 to :35) at callsite(\"trace.<locals>.trace_func.<locals>.func_wrapper\"(\"/tmp/ipython-input-2-2803309189.py\":48:12 to :23) at callsite(\"<lambda>\"(\"/tmp/ipython-input-18-2145028508.py\":2:19 to :69) at \"<module>\"(\"/tmp/ipython-input-18-2145028508.py\":1:7 to 3:29)))))))))))}, location_cache={(<code object multiply_add_prim at 0x781170701b30, file \"/tmp/ipython-input-4-2637569133.py\", line 5>, 54): loc(\"multiply_add_prim\"(\"/tmp/ipython-input-4-2637569133.py\":12:9 to :37)), (<code object func_wrapper at 0x781188cb96b0, file \"/tmp/ipython-input-2-2803309189.py\", line 45>, 98): loc(\"trace.<locals>.trace_func.<locals>.func_wrapper\"(\"/tmp/ipython-input-2-2803309189.py\":48:12 to :23)), (<code object multiply_add_value_and_jvp at 0x7811706fc570, file \"/tmp/ipython-input-16-347789876.py\", line 3>, 88): loc(\"multiply_add_value_and_jvp\"(\"/tmp/ipython-input-16-347789876.py\":27:15 to :41)), (<code object square_add_prim at 0x781170455290, file \"/tmp/ipython-input-4-2637569133.py\", line 14>, 32): loc(\"square_add_prim\"(\"/tmp/ipython-input-4-2637569133.py\":17:9 to :35)), (<code object <lambda> at 0x781170701130, file \"/tmp/ipython-input-18-2145028508.py\", line 1>, 64): loc(\"<lambda>\"(\"/tmp/ipython-input-18-2145028508.py\":2:19 to :69)), (<code object <cell line: 0> at 0x781170469ce0, file \"/tmp/ipython-input-18-2145028508.py\", line 1>, 54): loc(\"<module>\"(\"/tmp/ipython-input-18-2145028508.py\":1:7 to 3:29)), (<code object multiply_add_value_and_jvp at 0x7811706fc570, file \"/tmp/ipython-input-16-347789876.py\", line 3>, 232): loc(\"multiply_add_value_and_jvp\"(\"/tmp/ipython-input-16-347789876.py\":41:55 to :105)), (<code object multiply_add_value_and_jvp at 0x7811706fc570, file \"/tmp/ipython-input-16-347789876.py\", line 3>, 246): loc(\"multiply_add_value_and_jvp\"(\"/tmp/ipython-input-16-347789876.py\":41:19 to :106))}, canonical_name_cache={'/tmp/ipython-input-4-2637569133.py': '/tmp/ipython-input-4-2637569133.py', '/tmp/ipython-input-2-2803309189.py': '/tmp/ipython-input-2-2803309189.py', '/tmp/ipython-input-16-347789876.py': '/tmp/ipython-input-16-347789876.py', '/tmp/ipython-input-18-2145028508.py': '/tmp/ipython-input-18-2145028508.py'}, is_user_file_cache={'/usr/local/lib/python3.11/dist-packages/jax/_src/source_info_util.py': False, '/usr/local/lib/python3.11/dist-packages/jax/_src/interpreters/partial_eval.py': False, '/usr/local/lib/python3.11/dist-packages/jax/_src/core.py': False, '/tmp/ipython-input-4-2637569133.py': True, '/tmp/ipython-input-2-2803309189.py': True, '/tmp/ipython-input-16-347789876.py': True, '/usr/local/lib/python3.11/dist-packages/jax/_src/interpreters/ad.py': False, '/usr/local/lib/python3.11/dist-packages/jax/_src/linear_util.py': False, '/usr/local/lib/python3.11/dist-packages/jax/_src/api_util.py': False, '/usr/local/lib/python3.11/dist-packages/jax/_src/api.py': False, '/usr/local/lib/python3.11/dist-packages/jax/_src/traceback_util.py': False, '/tmp/ipython-input-18-2145028508.py': True, '/usr/local/lib/python3.11/dist-packages/jax/_src/profiler.py': False, '/usr/local/lib/python3.11/dist-packages/jax/_src/pjit.py': False}), lowering_parameters=LoweringParameters(override_lowering_rules=None, global_constant_computation=False, for_export=False, export_ignore_forward_compatibility=False)), name_stack=NameStack(stack=(Scope(name='jit(<lambda>)'), Scope(name='jit(main)'), Transform(name='jvp'))), primitive=multiply_add, avals_in=[ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True), ShapedArray(float32[])], avals_out=[ShapedArray(float32[])], tokens_in=<jax._src.interpreters.mlir.TokenSet object at 0x781170fd4290>, tokens_out=None, axis_size_env=None, dim_var_values=[], jaxpr_eqn_ctx=JaxprEqnContext(compute_type=None, threefry_partitionable=True, cur_abstract_mesh=AbstractMesh((), axis_types={}), xla_metadata=None), platforms=None), Value(<block argument> of type 'tensor<f32>' at index: 2), Value(<block argument> of type 'tensor<f32>' at index: 0), Value(%3 = \"stablehlo.add\"(%2, %arg3) : (tensor<f32>, tensor<f32>) -> tensor<f32>))\n",
            "|<- multiply_add_lowering = [<jaxlib.mlir._mlir_libs._mlir.ir.OpResult object at 0x781170fd6570>]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that first, you evaluate multiply_add_value_and_jvp abstractly, which in turn evaluates abstractly both the primal and the tangent evaluation (a total of 3 invocations of the ma primitive). Then, you compile the 3 occurrences of the primitive."
      ],
      "metadata": {
        "id": "dMz9rLNjwFbI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reverse differentiation"
      ],
      "metadata": {
        "id": "X0s84hTpwHnw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you attempt now to use reverse differentiation, you’ll notice that JAX starts by using the multiply_add_value_and_jvp to compute the forward differentiation for abstract values, but then runs into a NotImplementedError.\n",
        "\n",
        "When computing the reverse differentiation, JAX first performs an abstract evaluation of the forward differentiation code multiply_add_value_and_jvp to obtain a trace of primitives that compute the output tangent.\n",
        "\n",
        "1. Observe that JAX performs this abstract evaluation with concrete values for the differentiation point, and abstract values for the tangents.\n",
        "\n",
        "2. Notice that JAX uses the special abstract tangent value Zero for the tangent corresponding to the third argument of ma. This reflects the fact that you do not differentiate w.r.t. the second argument to square_add_prim, which flows to the third argument to multiply_add_prim.\n",
        "\n",
        "3. Notice also that during the abstract evaluation of the tangent you pass the value 0.0 as the tangent for the third argument. This is because of the use of the make_zero function in the definition of multiply_add_value_and_jvp."
      ],
      "metadata": {
        "id": "dC4ZpyLYwKYx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is reverse differentiation w.r.t. the first argument of `square_add_prim`\n",
        "with expectNotImplementedError():\n",
        "  api.grad(square_add_prim)(2., 10.)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3JMMop7wyWo",
        "outputId": "f63ed7ba-fff9-43f7-ad88-77470f0211b3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "call square_add_prim(Traced<ShapedArray(float32[], weak_type=True)>, 10.0)\n",
            "  call multiply_add_prim(Traced<ShapedArray(float32[], weak_type=True)>, Traced<ShapedArray(float32[], weak_type=True)>, 10.0)\n",
            "    call multiply_add_value_and_jvp((2.0, 2.0, 10.0), (Traced<ShapedArray(float32[], weak_type=True)>, Traced<ShapedArray(float32[], weak_type=True)>, Zero(ShapedArray(float32[], weak_type=True))))\n",
            "      Primal evaluation:\n",
            "      call multiply_add_prim(2.0, 2.0, 10.0)\n",
            "        call multiply_add_impl(2.0, 2.0, 10.0)\n",
            "        |<- multiply_add_impl = 14.0\n",
            "      |<- multiply_add_prim = 14.0\n",
            "      Tangent evaluation:\n",
            "      call multiply_add_prim(2.0, Traced<ShapedArray(float32[], weak_type=True)>, 0.0)\n",
            "        call multiply_add_abstract_eval(ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True))\n",
            "        |<- multiply_add_abstract_eval = ShapedArray(float32[])\n",
            "      |<- multiply_add_prim = Traced<ShapedArray(float32[])>\n",
            "      call multiply_add_prim(Traced<ShapedArray(float32[], weak_type=True)>, 2.0, Traced<ShapedArray(float32[])>)\n",
            "        call multiply_add_abstract_eval(ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True), ShapedArray(float32[]))\n",
            "        |<- multiply_add_abstract_eval = ShapedArray(float32[])\n",
            "      |<- multiply_add_prim = Traced<ShapedArray(float32[])>\n",
            "    |<- multiply_add_value_and_jvp = (14.0, Traced<ShapedArray(float32[])>)\n",
            "  |<- multiply_add_prim = Traced<ShapedArray(float32[])>\n",
            "|<- square_add_prim = Traced<ShapedArray(float32[])>\n",
            "\n",
            "Found expected exception:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/jax/_src/interpreters/ad.py\", line 406, in get_primitive_transpose\n",
            "    return primitive_transposes[p]\n",
            "           ~~~~~~~~~~~~~~~~~~~~^^^\n",
            "KeyError: multiply_add\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    ColabKernelApp.launch_instance()\n",
            "jax._src.source_info_util.JaxStackTraceBeforeTransformation: NotImplementedError: Transpose rule (for reverse-mode differentiation) for 'multiply_add' not implemented\n",
            "\n",
            "The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.\n",
            "\n",
            "--------------------\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-19-2155094905.py\", line 3, in <cell line: 0>\n",
            "    api.grad(square_add_prim)(2., 10.)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/jax/_src/traceback_util.py\", line 180, in reraise_with_filtered_traceback\n",
            "    return fun(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/jax/_src/api.py\", line 399, in grad_f\n",
            "    _, g = value_and_grad_f(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "NotImplementedError: Transpose rule (for reverse-mode differentiation) for 'multiply_add' not implemented\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above error is because there is a missing piece for JAX to be able to use the forward differentiation code to compute reverse differentiation."
      ],
      "metadata": {
        "id": "AdYePse4w0fw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transposition"
      ],
      "metadata": {
        "id": "oU29-fDOw6cX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As previously explained, when computing reverse differentiation, JAX obtains a trace of primitives that compute the tangent using forward differentiation. Then, JAX interprets this trace abstractly backwards and for each primitive it applies a transposition rule.\n",
        "\n",
        "To understand what is going on, consider a simpler example of the function f(x, y) = x * y + y. Assume, you need to differentiate at the point (2., 4.). JAX will produce the following JVP tangent calculation of ft from the tangents of the input xt and yt:\n",
        "\n",
        "   a = xt * 4.\n",
        "   b = 2. * yt\n",
        "   c = a + b\n",
        "   ft = c + yt\n",
        "By construction, the tangent calculation is always linear in the input tangents. The only non-linear operator that may arise in the tangent calculation is multiplication, but then one of the operands is constant.\n",
        "\n",
        "JAX will produce the reverse differentiation computation by processing the JVP computation backwards. For each operation in the tangent computation, it accumulates the cotangents of the variables used by the operation, using the cotangent of the result of the operation:\n",
        "\n",
        "  # Initialize cotangents of inputs and intermediate variables:\n",
        "  xct = yct = act = bct = cct = 0.\n",
        "  # Initialize cotangent of the output:\n",
        "  fct = 1.\n",
        "  # Process `ft = c + yt`:\n",
        "  cct += fct\n",
        "  yct += fct\n",
        "  # Process `c = a + b`:\n",
        "  act += cct\n",
        "  bct += cct\n",
        "  # Process `b = 2. * yt`:\n",
        "  yct += 2. * bct\n",
        "  # Process `a = xt * 4.`:\n",
        "  xct += act * 4.\n",
        "One can verify that this computation produces xct = 4. and yct = 3., which are the partial derivatives of the function f.\n",
        "\n",
        "JAX knows for each primitive that may appear in a JVP calculation how to transpose it. Conceptually, if the primitive p(x, y, z) is linear in the arguments y and z for a constant value of x, e.g., p(x, y, z) = y*cy + z*cz, then the transposition of the primitive is:\n",
        "\n",
        "p_transpose(out_ct, x, _, _) = (None, out_ct*cy, out_ct*cz)\n",
        "Notice that p_transpose takes the cotangent of the output of the primitive and a value corresponding to each argument of the primitive. For the linear arguments, the transposition gets an undefined _ value, and for the other arguments it gets the actual constants. The transposition returns a cotangent value for each argument of the primitive, with the value None returned for the constant arguments.\n",
        "\n",
        "In particular:\n",
        "\n",
        " add_transpose(out_ct, _, _) = (out_ct, out_ct)\n",
        " mult_transpose(out_ct, x, _) = (None, x * out_ct)\n",
        " mult_transpose(out_ct, _, y) = (out_ct * y, None)"
      ],
      "metadata": {
        "id": "ecVFA33JxULQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@trace(\"multiply_add_transpose\")\n",
        "def multiply_add_transpose(ct, x, y, z):\n",
        "  \"\"\"Evaluates the transpose of a linear primitive.\n",
        "\n",
        "  This method is only used when computing the backward gradient following\n",
        "  `value_and_jvp`, and is only needed for primitives that are used in the JVP\n",
        "  calculation for some other primitive. You need a transposition for `multiply_add_prim`,\n",
        "  because you have used `multiply_add_prim` in the computation of the `output_tangent` in\n",
        "  `multiply_add_value_and_jvp`.\n",
        "\n",
        "  In this case, multiply_add is not a linear primitive. However, it is used linearly\n",
        "  w.r.t. tangents in `multiply_add_value_and_jvp`:\n",
        "       `output_tangent(xt, yt, zt) = multiply_add_prim(xt, y, multiply_add_prim(x, yt, zt))`.\n",
        "\n",
        "  Always one of the first two multiplicative arguments is a constant.\n",
        "\n",
        "  Args:\n",
        "      ct: The cotangent of the output of the primitive.\n",
        "      x, y, z: The values of the arguments. The arguments that are used linearly\n",
        "        get an ad.UndefinedPrimal value. The other arguments get a constant\n",
        "        value.\n",
        "\n",
        "  Returns:\n",
        "      A tuple with the cotangent of the inputs, with the value None\n",
        "      corresponding to the constant arguments.\n",
        "  \"\"\"\n",
        "  if not ad.is_undefined_primal(x):\n",
        "    # This use of multiply_add is with a constant \"x\".\n",
        "    assert ad.is_undefined_primal(y)\n",
        "    ct_y = ad.Zero(y.aval) if type(ct) is ad.Zero else multiply_add_prim(x, ct, lax.zeros_like_array(x))\n",
        "    res = None, ct_y, ct\n",
        "  else:\n",
        "    # This use of multiply_add is with a constant \"y\".\n",
        "    assert ad.is_undefined_primal(x)\n",
        "    ct_x = ad.Zero(x.aval) if type(ct) is ad.Zero else multiply_add_prim(ct, y, lax.zeros_like_array(y))\n",
        "    res = ct_x, None, ct\n",
        "  return res\n",
        "\n",
        "ad.primitive_transposes[multiply_add_p] = multiply_add_transpose"
      ],
      "metadata": {
        "id": "U5fXNv8RxWGo"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you can complete the run of the grad:"
      ],
      "metadata": {
        "id": "NW46pfqDxZz3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assert api.grad(square_add_prim)(2., 10.) == 4."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXzA1Bxzxawv",
        "outputId": "2ebff0e5-2de7-4cec-bf54-daa1109a9ca2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "call square_add_prim(Traced<ShapedArray(float32[], weak_type=True)>, 10.0)\n",
            "  call multiply_add_prim(Traced<ShapedArray(float32[], weak_type=True)>, Traced<ShapedArray(float32[], weak_type=True)>, 10.0)\n",
            "    call multiply_add_value_and_jvp((2.0, 2.0, 10.0), (Traced<ShapedArray(float32[], weak_type=True)>, Traced<ShapedArray(float32[], weak_type=True)>, Zero(ShapedArray(float32[], weak_type=True))))\n",
            "      Primal evaluation:\n",
            "      call multiply_add_prim(2.0, 2.0, 10.0)\n",
            "        call multiply_add_impl(2.0, 2.0, 10.0)\n",
            "        |<- multiply_add_impl = 14.0\n",
            "      |<- multiply_add_prim = 14.0\n",
            "      Tangent evaluation:\n",
            "      call multiply_add_prim(2.0, Traced<ShapedArray(float32[], weak_type=True)>, 0.0)\n",
            "        call multiply_add_abstract_eval(ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True))\n",
            "        |<- multiply_add_abstract_eval = ShapedArray(float32[])\n",
            "      |<- multiply_add_prim = Traced<ShapedArray(float32[])>\n",
            "      call multiply_add_prim(Traced<ShapedArray(float32[], weak_type=True)>, 2.0, Traced<ShapedArray(float32[])>)\n",
            "        call multiply_add_abstract_eval(ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True), ShapedArray(float32[]))\n",
            "        |<- multiply_add_abstract_eval = ShapedArray(float32[])\n",
            "      |<- multiply_add_prim = Traced<ShapedArray(float32[])>\n",
            "    |<- multiply_add_value_and_jvp = (14.0, Traced<ShapedArray(float32[])>)\n",
            "  |<- multiply_add_prim = Traced<ShapedArray(float32[])>\n",
            "|<- square_add_prim = Traced<ShapedArray(float32[])>\n",
            "call multiply_add_transpose(1.0, UndefinedPrimal(ShapedArray(float32[], weak_type=True)), 2.0, UndefinedPrimal(ShapedArray(float32[])))\n",
            "  call multiply_add_prim(1.0, 2.0, 0.0)\n",
            "    call multiply_add_impl(1.0, 2.0, 0.0)\n",
            "    |<- multiply_add_impl = 2.0\n",
            "  |<- multiply_add_prim = 2.0\n",
            "|<- multiply_add_transpose = (2.0, None, 1.0)\n",
            "call multiply_add_transpose(1.0, 2.0, UndefinedPrimal(ShapedArray(float32[], weak_type=True)), 0.0)\n",
            "  call multiply_add_prim(2.0, 1.0, 0.0)\n",
            "    call multiply_add_impl(2.0, 1.0, 0.0)\n",
            "    |<- multiply_add_impl = 2.0\n",
            "  |<- multiply_add_prim = 2.0\n",
            "|<- multiply_add_transpose = (None, 2.0, 1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice the two calls to multiply_add_transpose. They correspond to the two uses of multiply_add_prim in the computation of the output_tangent in multiply_add_value_and_jvp. The first call to transpose corresponds to the last use of multiply_add_prim: multiply_add_prim(xt, y, ...) where y is the constant 2.0."
      ],
      "metadata": {
        "id": "HXON0Eurxdro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### JIT of reverse differentiation"
      ],
      "metadata": {
        "id": "C5wY9pbpxf1w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that the abstract evaluation of the multiply_add_value_and_jvp is using only abstract values. Meanwhile, in the absence of JIT, you used ConcreteArray."
      ],
      "metadata": {
        "id": "upSGdOJ2xhcw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assert api.jit(api.grad(square_add_prim))(2., 10.) == 4."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gq3IIpLixibn",
        "outputId": "0c3d88f6-2a84-4f1d-8aa9-0907ffe632b7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "call square_add_prim(Traced<ShapedArray(float32[], weak_type=True)>, Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace>)\n",
            "  call multiply_add_prim(Traced<ShapedArray(float32[], weak_type=True)>, Traced<ShapedArray(float32[], weak_type=True)>, Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace>)\n",
            "    call multiply_add_value_and_jvp((Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace>, Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace>, Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace>), (Traced<ShapedArray(float32[], weak_type=True)>, Traced<ShapedArray(float32[], weak_type=True)>, Zero(ShapedArray(float32[], weak_type=True))))\n",
            "      Primal evaluation:\n",
            "      call multiply_add_prim(Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace>, Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace>, Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace>)\n",
            "        call multiply_add_abstract_eval(ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True))\n",
            "        |<- multiply_add_abstract_eval = ShapedArray(float32[])\n",
            "      |<- multiply_add_prim = Traced<ShapedArray(float32[])>with<DynamicJaxprTrace>\n",
            "      Tangent evaluation:\n",
            "      call multiply_add_prim(Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace>, Traced<ShapedArray(float32[], weak_type=True)>, Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace>)\n",
            "        call multiply_add_abstract_eval(ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True))\n",
            "        |<- multiply_add_abstract_eval = ShapedArray(float32[])\n",
            "      |<- multiply_add_prim = Traced<ShapedArray(float32[])>\n",
            "      call multiply_add_prim(Traced<ShapedArray(float32[], weak_type=True)>, Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace>, Traced<ShapedArray(float32[])>)\n",
            "        call multiply_add_abstract_eval(ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True), ShapedArray(float32[]))\n",
            "        |<- multiply_add_abstract_eval = ShapedArray(float32[])\n",
            "      |<- multiply_add_prim = Traced<ShapedArray(float32[])>\n",
            "    |<- multiply_add_value_and_jvp = (Traced<ShapedArray(float32[])>with<DynamicJaxprTrace>, Traced<ShapedArray(float32[])>)\n",
            "  |<- multiply_add_prim = Traced<ShapedArray(float32[])>\n",
            "|<- square_add_prim = Traced<ShapedArray(float32[])>\n",
            "call multiply_add_transpose(Traced<ShapedArray(float32[])>with<DynamicJaxprTrace>, UndefinedPrimal(ShapedArray(float32[], weak_type=True)), Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace>, UndefinedPrimal(ShapedArray(float32[])))\n",
            "  call multiply_add_prim(Traced<ShapedArray(float32[])>with<DynamicJaxprTrace>, Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace>, Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace>)\n",
            "    call multiply_add_abstract_eval(ShapedArray(float32[]), ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True))\n",
            "    |<- multiply_add_abstract_eval = ShapedArray(float32[])\n",
            "  |<- multiply_add_prim = Traced<ShapedArray(float32[])>with<DynamicJaxprTrace>\n",
            "|<- multiply_add_transpose = (Traced<ShapedArray(float32[])>with<DynamicJaxprTrace>, None, Traced<ShapedArray(float32[])>with<DynamicJaxprTrace>)\n",
            "call multiply_add_transpose(Traced<ShapedArray(float32[])>with<DynamicJaxprTrace>, Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace>, UndefinedPrimal(ShapedArray(float32[], weak_type=True)), Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace>)\n",
            "  call multiply_add_prim(Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace>, Traced<ShapedArray(float32[])>with<DynamicJaxprTrace>, Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace>)\n",
            "    call multiply_add_abstract_eval(ShapedArray(float32[], weak_type=True), ShapedArray(float32[]), ShapedArray(float32[], weak_type=True))\n",
            "    |<- multiply_add_abstract_eval = ShapedArray(float32[])\n",
            "  |<- multiply_add_prim = Traced<ShapedArray(float32[])>with<DynamicJaxprTrace>\n",
            "|<- multiply_add_transpose = (None, Traced<ShapedArray(float32[])>with<DynamicJaxprTrace>, Traced<ShapedArray(float32[])>with<DynamicJaxprTrace>)\n",
            "call multiply_add_lowering(LoweringRuleContext(module_context=ModuleContext(context=<jax._src.interpreters.mlir.JaxIrContext object at 0x781171015a30>, module=<jaxlib.mlir._mlir_libs._mlir.ir.Module object at 0x7811706183b0>, ip=<jaxlib.mlir._mlir_libs._mlir.ir.InsertionPoint object at 0x781170618440>, symbol_table=<jaxlib.mlir._mlir_libs._mlir.ir.SymbolTable object at 0x78117116d2f0>, platforms=('cpu',), backend=<jaxlib.xla_extension.Client object at 0x7811712dae90>, axis_context=ShardingContext(num_devices=1, device_assignment=None, abstract_mesh=None), keepalives=[], channel_iterator=count(1), host_callbacks=[], shape_poly_state=<jax._src.interpreters.mlir.ShapePolyLoweringState object at 0x781170fd5e90>, all_default_mem_kind=True, cached_primitive_lowerings={}, traceback_caches=TracebackCaches(traceback_cache={<jaxlib.xla_extension.Traceback object at 0x2da153e0>: loc(callsite(\"multiply_add_prim\"(\"/tmp/ipython-input-4-2637569133.py\":12:9 to :37) at callsite(\"trace.<locals>.trace_func.<locals>.func_wrapper\"(\"/tmp/ipython-input-2-2803309189.py\":48:12 to :23) at callsite(\"multiply_add_value_and_jvp\"(\"/tmp/ipython-input-16-347789876.py\":41:19 to :106) at callsite(\"trace.<locals>.trace_func.<locals>.func_wrapper\"(\"/tmp/ipython-input-2-2803309189.py\":48:12 to :23) at callsite(\"multiply_add_prim\"(\"/tmp/ipython-input-4-2637569133.py\":12:9 to :37) at callsite(\"trace.<locals>.trace_func.<locals>.func_wrapper\"(\"/tmp/ipython-input-2-2803309189.py\":48:12 to :23) at callsite(\"square_add_prim\"(\"/tmp/ipython-input-4-2637569133.py\":17:9 to :35) at callsite(\"trace.<locals>.trace_func.<locals>.func_wrapper\"(\"/tmp/ipython-input-2-2803309189.py\":48:12 to :23) at callsite(\"<module>\"(\"/tmp/ipython-input-22-3085343041.py\":1:7 to :50) at \"InteractiveShell.run_code\"(\"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\":3553:20 to :69)))))))))))}, location_cache={(<code object multiply_add_prim at 0x781170701b30, file \"/tmp/ipython-input-4-2637569133.py\", line 5>, 54): loc(\"multiply_add_prim\"(\"/tmp/ipython-input-4-2637569133.py\":12:9 to :37)), (<code object func_wrapper at 0x781188cb96b0, file \"/tmp/ipython-input-2-2803309189.py\", line 45>, 98): loc(\"trace.<locals>.trace_func.<locals>.func_wrapper\"(\"/tmp/ipython-input-2-2803309189.py\":48:12 to :23)), (<code object multiply_add_value_and_jvp at 0x7811706fc570, file \"/tmp/ipython-input-16-347789876.py\", line 3>, 246): loc(\"multiply_add_value_and_jvp\"(\"/tmp/ipython-input-16-347789876.py\":41:19 to :106)), (<code object square_add_prim at 0x781170455290, file \"/tmp/ipython-input-4-2637569133.py\", line 14>, 32): loc(\"square_add_prim\"(\"/tmp/ipython-input-4-2637569133.py\":17:9 to :35)), (<code object <cell line: 0> at 0x78117108b930, file \"/tmp/ipython-input-22-3085343041.py\", line 1>, 90): loc(\"<module>\"(\"/tmp/ipython-input-22-3085343041.py\":1:7 to :50)), (<code object run_code at 0x2ac2a720, file \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3512>, 426): loc(\"InteractiveShell.run_code\"(\"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\":3553:20 to :69))}, canonical_name_cache={'/tmp/ipython-input-4-2637569133.py': '/tmp/ipython-input-4-2637569133.py', '/tmp/ipython-input-2-2803309189.py': '/tmp/ipython-input-2-2803309189.py', '/tmp/ipython-input-16-347789876.py': '/tmp/ipython-input-16-347789876.py', '/tmp/ipython-input-22-3085343041.py': '/tmp/ipython-input-22-3085343041.py', '/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py': '/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py'}, is_user_file_cache={'/usr/local/lib/python3.11/dist-packages/jax/_src/source_info_util.py': False, '/usr/local/lib/python3.11/dist-packages/jax/_src/interpreters/partial_eval.py': False, '/usr/local/lib/python3.11/dist-packages/jax/_src/core.py': False, '/tmp/ipython-input-4-2637569133.py': True, '/tmp/ipython-input-2-2803309189.py': True, '/tmp/ipython-input-16-347789876.py': True, '/usr/local/lib/python3.11/dist-packages/jax/_src/interpreters/ad.py': False, '/usr/local/lib/python3.11/dist-packages/jax/_src/linear_util.py': False, '/usr/local/lib/python3.11/dist-packages/jax/_src/api_util.py': False, '/usr/local/lib/python3.11/dist-packages/jax/_src/profiler.py': False, '/usr/local/lib/python3.11/dist-packages/jax/_src/api.py': False, '/usr/local/lib/python3.11/dist-packages/jax/_src/traceback_util.py': False, '/usr/local/lib/python3.11/dist-packages/jax/_src/pjit.py': False, '/tmp/ipython-input-22-3085343041.py': True, '/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py': True}), lowering_parameters=LoweringParameters(override_lowering_rules=None, global_constant_computation=False, for_export=False, export_ignore_forward_compatibility=False)), name_stack=NameStack(stack=(Scope(name='jit(square_add_prim)'), Scope(name='jit(main)'), Transform(name='transpose'), Transform(name='jvp'))), primitive=multiply_add, avals_in=[ShapedArray(float32[]), ShapedArray(float32[], weak_type=True), ShapedArray(float32[], weak_type=True)], avals_out=[ShapedArray(float32[])], tokens_in=<jax._src.interpreters.mlir.TokenSet object at 0x78117101b110>, tokens_out=None, axis_size_env=None, dim_var_values=[], jaxpr_eqn_ctx=JaxprEqnContext(compute_type=None, threefry_partitionable=True, cur_abstract_mesh=AbstractMesh((), axis_types={}), xla_metadata=None), platforms=None), Value(%0 = \"stablehlo.constant\"() <{value = dense<1.000000e+00> : tensor<f32>}> : () -> tensor<f32>), Value(<block argument> of type 'tensor<f32>' at index: 0), Value(%1 = \"stablehlo.constant\"() <{value = dense<0.000000e+00> : tensor<f32>}> : () -> tensor<f32>))\n",
            "|<- multiply_add_lowering = [<jaxlib.mlir._mlir_libs._mlir.ir.OpResult object at 0x78117101b9b0>]\n",
            "call multiply_add_lowering(LoweringRuleContext(module_context=ModuleContext(context=<jax._src.interpreters.mlir.JaxIrContext object at 0x781171015a30>, module=<jaxlib.mlir._mlir_libs._mlir.ir.Module object at 0x7811706183b0>, ip=<jaxlib.mlir._mlir_libs._mlir.ir.InsertionPoint object at 0x781170618440>, symbol_table=<jaxlib.mlir._mlir_libs._mlir.ir.SymbolTable object at 0x78117116d2f0>, platforms=('cpu',), backend=<jaxlib.xla_extension.Client object at 0x7811712dae90>, axis_context=ShardingContext(num_devices=1, device_assignment=None, abstract_mesh=None), keepalives=[], channel_iterator=count(1), host_callbacks=[], shape_poly_state=<jax._src.interpreters.mlir.ShapePolyLoweringState object at 0x781170fd5e90>, all_default_mem_kind=True, cached_primitive_lowerings={}, traceback_caches=TracebackCaches(traceback_cache={<jaxlib.xla_extension.Traceback object at 0x2da153e0>: loc(callsite(\"multiply_add_prim\"(\"/tmp/ipython-input-4-2637569133.py\":12:9 to :37) at callsite(\"trace.<locals>.trace_func.<locals>.func_wrapper\"(\"/tmp/ipython-input-2-2803309189.py\":48:12 to :23) at callsite(\"multiply_add_value_and_jvp\"(\"/tmp/ipython-input-16-347789876.py\":41:19 to :106) at callsite(\"trace.<locals>.trace_func.<locals>.func_wrapper\"(\"/tmp/ipython-input-2-2803309189.py\":48:12 to :23) at callsite(\"multiply_add_prim\"(\"/tmp/ipython-input-4-2637569133.py\":12:9 to :37) at callsite(\"trace.<locals>.trace_func.<locals>.func_wrapper\"(\"/tmp/ipython-input-2-2803309189.py\":48:12 to :23) at callsite(\"square_add_prim\"(\"/tmp/ipython-input-4-2637569133.py\":17:9 to :35) at callsite(\"trace.<locals>.trace_func.<locals>.func_wrapper\"(\"/tmp/ipython-input-2-2803309189.py\":48:12 to :23) at callsite(\"<module>\"(\"/tmp/ipython-input-22-3085343041.py\":1:7 to :50) at \"InteractiveShell.run_code\"(\"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\":3553:20 to :69))))))))))), <jaxlib.xla_extension.Traceback object at 0x2db59330>: loc(callsite(\"multiply_add_prim\"(\"/tmp/ipython-input-4-2637569133.py\":12:9 to :37) at callsite(\"trace.<locals>.trace_func.<locals>.func_wrapper\"(\"/tmp/ipython-input-2-2803309189.py\":48:12 to :23) at callsite(\"multiply_add_value_and_jvp\"(\"/tmp/ipython-input-16-347789876.py\":41:55 to :105) at callsite(\"trace.<locals>.trace_func.<locals>.func_wrapper\"(\"/tmp/ipython-input-2-2803309189.py\":48:12 to :23) at callsite(\"multiply_add_prim\"(\"/tmp/ipython-input-4-2637569133.py\":12:9 to :37) at callsite(\"trace.<locals>.trace_func.<locals>.func_wrapper\"(\"/tmp/ipython-input-2-2803309189.py\":48:12 to :23) at callsite(\"square_add_prim\"(\"/tmp/ipython-input-4-2637569133.py\":17:9 to :35) at callsite(\"trace.<locals>.trace_func.<locals>.func_wrapper\"(\"/tmp/ipython-input-2-2803309189.py\":48:12 to :23) at callsite(\"<module>\"(\"/tmp/ipython-input-22-3085343041.py\":1:7 to :50) at \"InteractiveShell.run_code\"(\"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\":3553:20 to :69)))))))))))}, location_cache={(<code object multiply_add_prim at 0x781170701b30, file \"/tmp/ipython-input-4-2637569133.py\", line 5>, 54): loc(\"multiply_add_prim\"(\"/tmp/ipython-input-4-2637569133.py\":12:9 to :37)), (<code object func_wrapper at 0x781188cb96b0, file \"/tmp/ipython-input-2-2803309189.py\", line 45>, 98): loc(\"trace.<locals>.trace_func.<locals>.func_wrapper\"(\"/tmp/ipython-input-2-2803309189.py\":48:12 to :23)), (<code object multiply_add_value_and_jvp at 0x7811706fc570, file \"/tmp/ipython-input-16-347789876.py\", line 3>, 246): loc(\"multiply_add_value_and_jvp\"(\"/tmp/ipython-input-16-347789876.py\":41:19 to :106)), (<code object square_add_prim at 0x781170455290, file \"/tmp/ipython-input-4-2637569133.py\", line 14>, 32): loc(\"square_add_prim\"(\"/tmp/ipython-input-4-2637569133.py\":17:9 to :35)), (<code object <cell line: 0> at 0x78117108b930, file \"/tmp/ipython-input-22-3085343041.py\", line 1>, 90): loc(\"<module>\"(\"/tmp/ipython-input-22-3085343041.py\":1:7 to :50)), (<code object run_code at 0x2ac2a720, file \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3512>, 426): loc(\"InteractiveShell.run_code\"(\"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\":3553:20 to :69)), (<code object multiply_add_value_and_jvp at 0x7811706fc570, file \"/tmp/ipython-input-16-347789876.py\", line 3>, 232): loc(\"multiply_add_value_and_jvp\"(\"/tmp/ipython-input-16-347789876.py\":41:55 to :105))}, canonical_name_cache={'/tmp/ipython-input-4-2637569133.py': '/tmp/ipython-input-4-2637569133.py', '/tmp/ipython-input-2-2803309189.py': '/tmp/ipython-input-2-2803309189.py', '/tmp/ipython-input-16-347789876.py': '/tmp/ipython-input-16-347789876.py', '/tmp/ipython-input-22-3085343041.py': '/tmp/ipython-input-22-3085343041.py', '/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py': '/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py'}, is_user_file_cache={'/usr/local/lib/python3.11/dist-packages/jax/_src/source_info_util.py': False, '/usr/local/lib/python3.11/dist-packages/jax/_src/interpreters/partial_eval.py': False, '/usr/local/lib/python3.11/dist-packages/jax/_src/core.py': False, '/tmp/ipython-input-4-2637569133.py': True, '/tmp/ipython-input-2-2803309189.py': True, '/tmp/ipython-input-16-347789876.py': True, '/usr/local/lib/python3.11/dist-packages/jax/_src/interpreters/ad.py': False, '/usr/local/lib/python3.11/dist-packages/jax/_src/linear_util.py': False, '/usr/local/lib/python3.11/dist-packages/jax/_src/api_util.py': False, '/usr/local/lib/python3.11/dist-packages/jax/_src/profiler.py': False, '/usr/local/lib/python3.11/dist-packages/jax/_src/api.py': False, '/usr/local/lib/python3.11/dist-packages/jax/_src/traceback_util.py': False, '/usr/local/lib/python3.11/dist-packages/jax/_src/pjit.py': False, '/tmp/ipython-input-22-3085343041.py': True, '/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py': True}), lowering_parameters=LoweringParameters(override_lowering_rules=None, global_constant_computation=False, for_export=False, export_ignore_forward_compatibility=False)), name_stack=NameStack(stack=(Scope(name='jit(square_add_prim)'), Scope(name='jit(main)'), Transform(name='transpose'), Transform(name='jvp'))), primitive=multiply_add, avals_in=[ShapedArray(float32[], weak_type=True), ShapedArray(float32[]), ShapedArray(float32[], weak_type=True)], avals_out=[ShapedArray(float32[])], tokens_in=<jax._src.interpreters.mlir.TokenSet object at 0x781171018990>, tokens_out=None, axis_size_env=None, dim_var_values=[], jaxpr_eqn_ctx=JaxprEqnContext(compute_type=None, threefry_partitionable=True, cur_abstract_mesh=AbstractMesh((), axis_types={}), xla_metadata=None), platforms=None), Value(<block argument> of type 'tensor<f32>' at index: 0), Value(%0 = \"stablehlo.constant\"() <{value = dense<1.000000e+00> : tensor<f32>}> : () -> tensor<f32>), Value(%1 = \"stablehlo.constant\"() <{value = dense<0.000000e+00> : tensor<f32>}> : () -> tensor<f32>))\n",
            "|<- multiply_add_lowering = [<jaxlib.mlir._mlir_libs._mlir.ir.OpResult object at 0x78117101b730>]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batching"
      ],
      "metadata": {
        "id": "9Y2WYYaCxvp4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The batching transformation takes a point-wise computation and turns it into a computation on vectors. If you try it right now, you will get a NotImplementedError:"
      ],
      "metadata": {
        "id": "8-_aN9hnxxNw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The arguments are two vectors instead of two scalars.\n",
        "with expectNotImplementedError():\n",
        "  api.vmap(square_add_prim, in_axes=0, out_axes=0)(np.array([2., 3.]),\n",
        "                                               np.array([10., 20.]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1lPxua3xyXA",
        "outputId": "fc6ebf0c-7266-4f5b-a513-9eec713e0441"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "call square_add_prim(Traced<ShapedArray(float32[])>, Traced<ShapedArray(float32[])>)\n",
            "  call multiply_add_prim(Traced<ShapedArray(float32[])>, Traced<ShapedArray(float32[])>, Traced<ShapedArray(float32[])>)\n",
            "\n",
            "Found expected exception:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-23-1080163607.py\", line 3, in <cell line: 0>\n",
            "    api.vmap(square_add_prim, in_axes=0, out_axes=0)(np.array([2., 3.]),\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/jax/_src/traceback_util.py\", line 180, in reraise_with_filtered_traceback\n",
            "    return fun(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/jax/_src/api.py\", line 1015, in vmap_f\n",
            "    out_flat = batching.batch(\n",
            "               ^^^^^^^^^^^^^^^\n",
            "NotImplementedError: Batching rule for 'multiply_add' not implemented\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You need to instruct JAX how to evaluate the batched version of the primitive. In this particular case, the multiply_add_prim already operates pointwise for any dimension of input vectors, so the batched version can use the same multiply_add_prim implementation."
      ],
      "metadata": {
        "id": "samZ-T6Yx0bI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from jax.interpreters import batching\n",
        "\n",
        "@trace(\"multiply_add_batch\")\n",
        "def multiply_add_batch(vector_arg_values, batch_axes):\n",
        "  \"\"\"Computes the batched version of the primitive.\n",
        "\n",
        "  This must be a JAX-traceable function.\n",
        "\n",
        "  Since the `multiply_add primitive` already operates point-wise on arbitrary\n",
        "  dimension tensors, to batch it you can use the primitive itself. This works as\n",
        "  long as both the inputs have the same dimensions and are batched along the\n",
        "  same axes. The result is batched along the axis that the inputs are batched.\n",
        "\n",
        "  Args:\n",
        "    vector_arg_values: A tuple of two arguments, each being a tensor of matching\n",
        "      shape.\n",
        "    batch_axes: The axes that are being batched. See vmap documentation.\n",
        "\n",
        "  Returns:\n",
        "    A tuple of the result, and the result axis that was batched.\n",
        "  \"\"\"\n",
        "  assert batch_axes[0] == batch_axes[1]\n",
        "  assert batch_axes[0] == batch_axes[2]\n",
        "  _trace(\"Using multiply_add to compute the batch:\")\n",
        "  res = multiply_add_prim(*vector_arg_values)\n",
        "  return res, batch_axes[0]\n",
        "\n",
        "\n",
        "batching.primitive_batchers[multiply_add_p] = multiply_add_batch"
      ],
      "metadata": {
        "id": "6Jj9ZIrex1cg"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert np.allclose(api.vmap(square_add_prim, in_axes=0, out_axes=0)(\n",
        "  np.array([2., 3.]),\n",
        "  np.array([10., 20.])),\n",
        "  [14., 29.])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmXz6jNKx3bv",
        "outputId": "ed948409-759c-4521-b2d4-40140c1242ec"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "call square_add_prim(Traced<ShapedArray(float32[])>, Traced<ShapedArray(float32[])>)\n",
            "  call multiply_add_prim(Traced<ShapedArray(float32[])>, Traced<ShapedArray(float32[])>, Traced<ShapedArray(float32[])>)\n",
            "    call multiply_add_batch(([2. 3.], [2. 3.], [10. 20.]), (0, 0, 0))\n",
            "      Using multiply_add to compute the batch:\n",
            "      call multiply_add_prim([2. 3.], [2. 3.], [10. 20.])\n",
            "        call multiply_add_impl([2. 3.], [2. 3.], [10. 20.])\n",
            "        |<- multiply_add_impl = [14. 29.]\n",
            "      |<- multiply_add_prim = [14. 29.]\n",
            "    |<- multiply_add_batch = ([14. 29.], 0)\n",
            "  |<- multiply_add_prim = Traced<ShapedArray(float32[])>\n",
            "|<- square_add_prim = Traced<ShapedArray(float32[])>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### JIT of batching"
      ],
      "metadata": {
        "id": "YsbfDuzox5X_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is an example of applying JIT to batching:"
      ],
      "metadata": {
        "id": "87tbLojqx6_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assert np.allclose(api.jit(api.vmap(square_add_prim, in_axes=0, out_axes=0))\n",
        "                    (np.array([2., 3.]),\n",
        "                     np.array([10., 20.])),\n",
        "                    [14., 29.])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IctHejrsx7yf",
        "outputId": "1747f78c-cc14-4d68-9787-7631893655bf"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "call square_add_prim(Traced<ShapedArray(float32[])>, Traced<ShapedArray(float32[])>)\n",
            "  call multiply_add_prim(Traced<ShapedArray(float32[])>, Traced<ShapedArray(float32[])>, Traced<ShapedArray(float32[])>)\n",
            "    call multiply_add_batch((Traced<ShapedArray(float32[2])>with<DynamicJaxprTrace>, Traced<ShapedArray(float32[2])>with<DynamicJaxprTrace>, Traced<ShapedArray(float32[2])>with<DynamicJaxprTrace>), (0, 0, 0))\n",
            "      Using multiply_add to compute the batch:\n",
            "      call multiply_add_prim(Traced<ShapedArray(float32[2])>with<DynamicJaxprTrace>, Traced<ShapedArray(float32[2])>with<DynamicJaxprTrace>, Traced<ShapedArray(float32[2])>with<DynamicJaxprTrace>)\n",
            "        call multiply_add_abstract_eval(ShapedArray(float32[2]), ShapedArray(float32[2]), ShapedArray(float32[2]))\n",
            "        |<- multiply_add_abstract_eval = ShapedArray(float32[2])\n",
            "      |<- multiply_add_prim = Traced<ShapedArray(float32[2])>with<DynamicJaxprTrace>\n",
            "    |<- multiply_add_batch = (Traced<ShapedArray(float32[2])>with<DynamicJaxprTrace>, 0)\n",
            "  |<- multiply_add_prim = Traced<ShapedArray(float32[])>\n",
            "|<- square_add_prim = Traced<ShapedArray(float32[])>\n",
            "call multiply_add_lowering(LoweringRuleContext(module_context=ModuleContext(context=<jax._src.interpreters.mlir.JaxIrContext object at 0x7811710159a0>, module=<jaxlib.mlir._mlir_libs._mlir.ir.Module object at 0x7811705fcbd0>, ip=<jaxlib.mlir._mlir_libs._mlir.ir.InsertionPoint object at 0x7811705fc350>, symbol_table=<jaxlib.mlir._mlir_libs._mlir.ir.SymbolTable object at 0x78117130b810>, platforms=('cpu',), backend=<jaxlib.xla_extension.Client object at 0x7811712dae90>, axis_context=ShardingContext(num_devices=1, device_assignment=None, abstract_mesh=None), keepalives=[], channel_iterator=count(1), host_callbacks=[], shape_poly_state=<jax._src.interpreters.mlir.ShapePolyLoweringState object at 0x78117101b810>, all_default_mem_kind=True, cached_primitive_lowerings={}, traceback_caches=TracebackCaches(traceback_cache={<jaxlib.xla_extension.Traceback object at 0x2dc57710>: loc(callsite(\"multiply_add_prim\"(\"/tmp/ipython-input-4-2637569133.py\":12:9 to :37) at callsite(\"trace.<locals>.trace_func.<locals>.func_wrapper\"(\"/tmp/ipython-input-2-2803309189.py\":48:12 to :23) at callsite(\"multiply_add_batch\"(\"/tmp/ipython-input-24-1827752256.py\":25:8 to :45) at callsite(\"trace.<locals>.trace_func.<locals>.func_wrapper\"(\"/tmp/ipython-input-2-2803309189.py\":48:12 to :23) at callsite(\"multiply_add_prim\"(\"/tmp/ipython-input-4-2637569133.py\":12:9 to :37) at callsite(\"trace.<locals>.trace_func.<locals>.func_wrapper\"(\"/tmp/ipython-input-2-2803309189.py\":48:12 to :23) at callsite(\"square_add_prim\"(\"/tmp/ipython-input-4-2637569133.py\":17:9 to :35) at callsite(\"trace.<locals>.trace_func.<locals>.func_wrapper\"(\"/tmp/ipython-input-2-2803309189.py\":48:12 to :23) at callsite(\"<module>\"(\"/tmp/ipython-input-26-1392464762.py\":1:19 to 3:42) at \"InteractiveShell.run_code\"(\"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\":3553:20 to :69)))))))))))}, location_cache={(<code object multiply_add_prim at 0x781170701b30, file \"/tmp/ipython-input-4-2637569133.py\", line 5>, 54): loc(\"multiply_add_prim\"(\"/tmp/ipython-input-4-2637569133.py\":12:9 to :37)), (<code object func_wrapper at 0x781188cb96b0, file \"/tmp/ipython-input-2-2803309189.py\", line 45>, 98): loc(\"trace.<locals>.trace_func.<locals>.func_wrapper\"(\"/tmp/ipython-input-2-2803309189.py\":48:12 to :23)), (<code object multiply_add_batch at 0x781171161140, file \"/tmp/ipython-input-24-1827752256.py\", line 3>, 126): loc(\"multiply_add_batch\"(\"/tmp/ipython-input-24-1827752256.py\":25:8 to :45)), (<code object square_add_prim at 0x781170455290, file \"/tmp/ipython-input-4-2637569133.py\", line 14>, 32): loc(\"square_add_prim\"(\"/tmp/ipython-input-4-2637569133.py\":17:9 to :35)), (<code object <cell line: 0> at 0x78117119d2c0, file \"/tmp/ipython-input-26-1392464762.py\", line 1>, 204): loc(\"<module>\"(\"/tmp/ipython-input-26-1392464762.py\":1:19 to 3:42)), (<code object run_code at 0x2ac2a720, file \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3512>, 426): loc(\"InteractiveShell.run_code\"(\"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\":3553:20 to :69))}, canonical_name_cache={'/tmp/ipython-input-4-2637569133.py': '/tmp/ipython-input-4-2637569133.py', '/tmp/ipython-input-2-2803309189.py': '/tmp/ipython-input-2-2803309189.py', '/tmp/ipython-input-24-1827752256.py': '/tmp/ipython-input-24-1827752256.py', '/tmp/ipython-input-26-1392464762.py': '/tmp/ipython-input-26-1392464762.py', '/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py': '/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py'}, is_user_file_cache={'/usr/local/lib/python3.11/dist-packages/jax/_src/source_info_util.py': False, '/usr/local/lib/python3.11/dist-packages/jax/_src/interpreters/partial_eval.py': False, '/usr/local/lib/python3.11/dist-packages/jax/_src/core.py': False, '/tmp/ipython-input-4-2637569133.py': True, '/tmp/ipython-input-2-2803309189.py': True, '/tmp/ipython-input-24-1827752256.py': True, '/usr/local/lib/python3.11/dist-packages/jax/_src/interpreters/batching.py': False, '/usr/local/lib/python3.11/dist-packages/jax/_src/linear_util.py': False, '/usr/local/lib/python3.11/dist-packages/jax/_src/api.py': False, '/usr/local/lib/python3.11/dist-packages/jax/_src/traceback_util.py': False, '/usr/local/lib/python3.11/dist-packages/jax/_src/api_util.py': False, '/usr/local/lib/python3.11/dist-packages/jax/_src/profiler.py': False, '/usr/local/lib/python3.11/dist-packages/jax/_src/pjit.py': False, '/tmp/ipython-input-26-1392464762.py': True, '/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py': True}), lowering_parameters=LoweringParameters(override_lowering_rules=None, global_constant_computation=False, for_export=False, export_ignore_forward_compatibility=False)), name_stack=NameStack(stack=(Scope(name='jit(square_add_prim)'), Scope(name='jit(main)'), Transform(name='vmap'))), primitive=multiply_add, avals_in=[ShapedArray(float32[2]), ShapedArray(float32[2]), ShapedArray(float32[2])], avals_out=[ShapedArray(float32[2])], tokens_in=<jax._src.interpreters.mlir.TokenSet object at 0x781170611bd0>, tokens_out=None, axis_size_env=None, dim_var_values=[], jaxpr_eqn_ctx=JaxprEqnContext(compute_type=None, threefry_partitionable=True, cur_abstract_mesh=AbstractMesh((), axis_types={}), xla_metadata=None), platforms=None), Value(<block argument> of type 'tensor<2xf32>' at index: 0), Value(<block argument> of type 'tensor<2xf32>' at index: 0), Value(<block argument> of type 'tensor<2xf32>' at index: 1))\n",
            "|<- multiply_add_lowering = [<jaxlib.mlir._mlir_libs._mlir.ir.OpResult object at 0x781170612c30>]\n"
          ]
        }
      ]
    }
  ]
}