{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This tutorial serves as an introduction to device parallelism for Single-Program Multi-Data (SPMD) code in JAX. SPMD is a parallelism technique where the same computation, such as the forward pass of a neural network, can be run on different input data (for example, different inputs in a batch) in parallel on different devices, such as several GPUs or Google TPUs.\n",
        "\n",
        "The tutorial covers three modes of parallel computation:\n",
        "\n",
        "1. Automatic sharding via jax.jit(): The compiler chooses the optimal computation strategy (a.k.a. “the compiler takes the wheel”).\n",
        "\n",
        "2. Explicit Sharding (*new*) is similar to automatic sharding in that you’re writing a global-view program. The difference is that the sharding of each array is part of the array’s JAX-level type making it an explicit part of the programming model. These shardings are propagated at the JAX level and queryable at trace time. It’s still the compiler’s responsibility to turn the whole-array program into per-device programs (turning jnp.sum into psum for example) but the compiler is heavily constrained by the user-supplied shardings.\n",
        "\n",
        "3. Fully manual sharding with manual control using jax.shard_map(): shard_map enables per-device code and explicit communication collectives"
      ],
      "metadata": {
        "id": "5D4fSwhtmF_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "jax.config.update('jax_num_cpu_devices', 8)\n",
        "\n",
        "jax.devices()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKlag5G1mPW5",
        "outputId": "a95329e0-95f7-4f03-dafb-55b10929a158"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[CpuDevice(id=0),\n",
              " CpuDevice(id=1),\n",
              " CpuDevice(id=2),\n",
              " CpuDevice(id=3),\n",
              " CpuDevice(id=4),\n",
              " CpuDevice(id=5),\n",
              " CpuDevice(id=6),\n",
              " CpuDevice(id=7)]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Key concept: Data sharding"
      ],
      "metadata": {
        "id": "6t6u0s4Vm3v5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key to all of the distributed computation approaches below is the concept of data sharding, which describes how data is laid out on the available devices.\n",
        "\n",
        "How can JAX understand how the data is laid out across devices? JAX’s datatype, the jax.Array immutable array data structure, represents arrays with physical storage spanning one or multiple devices, and helps make parallelism a core feature of JAX. The jax.Array object is designed with distributed data and computation in mind. Every jax.Array has an associated jax.sharding.Sharding object, which describes which shard of the global data is required by each global device. When you create a jax.Array from scratch, you also need to create its Sharding.\n",
        "\n",
        "In the simplest cases, arrays are sharded on a single device, as demonstrated below:"
      ],
      "metadata": {
        "id": "d5PYYs_2m6YJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import jax.numpy as jnp\n",
        "\n",
        "arr = jnp.arange(32.0).reshape(4, 8)\n",
        "print(arr)\n",
        "arr.devices()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xSBfj51m75I",
        "outputId": "1c457653-3c62-4af5-fdcf-8eb02df27972"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.  1.  2.  3.  4.  5.  6.  7.]\n",
            " [ 8.  9. 10. 11. 12. 13. 14. 15.]\n",
            " [16. 17. 18. 19. 20. 21. 22. 23.]\n",
            " [24. 25. 26. 27. 28. 29. 30. 31.]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{CpuDevice(id=0)}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "arr.sharding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-u33m-rm_TA",
        "outputId": "aa789180-0940-4402-de58-6f62ba4e66b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SingleDeviceSharding(device=CpuDevice(id=0), memory_kind=unpinned_host)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a more visual representation of the storage layout, the jax.debug module provides some helpers to visualize the sharding of an array. For example, jax.debug.visualize_array_sharding() displays how the array is stored in memory of a single device:"
      ],
      "metadata": {
        "id": "eYkgxEsLpqEn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "jax.debug.visualize_array_sharding(arr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "E6UIFt9UprIf",
        "outputId": "a8e69748-1dd4-44a1-f275-31dd4402e878"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;255;255;255;48;2;57;59;121m                                                  \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m                                                  \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m                                                  \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m                                                  \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m                                                  \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m                      \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121mCPU 0\u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m                       \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m                                                  \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m                                                  \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m                                                  \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m                                                  \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m                                                  \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                                                  </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                                                  </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                                                  </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                                                  </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                                                  </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                      CPU 0                       </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                                                  </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                                                  </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                                                  </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                                                  </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                                                  </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To create an array with a non-trivial sharding, you can define a jax.sharding specification for the array and pass this to jax.device_put().\n",
        "\n",
        "Here, define a NamedSharding, which specifies an N-dimensional grid of devices with named axes, where jax.sharding.Mesh allows for precise device placement:"
      ],
      "metadata": {
        "id": "_lIgk1MlpuWP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from jax.sharding import PartitionSpec as P\n",
        "\n",
        "mesh = jax.make_mesh((2, 4), ('x', 'y'))\n",
        "sharding = jax.sharding.NamedSharding(mesh, P('x', 'y'))\n",
        "print(sharding)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2mS-6tCpvZn",
        "outputId": "1a0520c1-fb85-4029-b1a3-1ad98fc89778"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NamedSharding(mesh=Mesh('x': 2, 'y': 4), spec=PartitionSpec('x', 'y'), memory_kind=unpinned_host)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NamedSharding(mesh=Mesh('x': 2, 'y': 4, axis_types=(Auto, Auto)), spec=PartitionSpec('x', 'y'), memory_kind=unpinned_host)\n",
        "Passing this Sharding object to jax.device_put(), you can obtain a sharded array:"
      ],
      "metadata": {
        "id": "7xnq6IThqU-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "arr_sharded = jax.device_put(arr, sharding)\n",
        "\n",
        "print(arr_sharded)\n",
        "jax.debug.visualize_array_sharding(arr_sharded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "4zH9rLoiqV6n",
        "outputId": "a1d073cf-05c8-48a5-fb5a-2b419e095386"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.  1.  2.  3.  4.  5.  6.  7.]\n",
            " [ 8.  9. 10. 11. 12. 13. 14. 15.]\n",
            " [16. 17. 18. 19. 20. 21. 22. 23.]\n",
            " [24. 25. 26. 27. 28. 29. 30. 31.]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;214;97;107m            \u001b[0m\u001b[38;2;255;255;255;48;2;140;162;82m            \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;214;97;107m            \u001b[0m\u001b[38;2;255;255;255;48;2;140;162;82m            \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m   \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121mCPU 0\u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m    \u001b[0m\u001b[38;2;255;255;255;48;2;214;97;107m   \u001b[0m\u001b[38;2;255;255;255;48;2;214;97;107mCPU 1\u001b[0m\u001b[38;2;255;255;255;48;2;214;97;107m    \u001b[0m\u001b[38;2;255;255;255;48;2;140;162;82m   \u001b[0m\u001b[38;2;255;255;255;48;2;140;162;82mCPU 2\u001b[0m\u001b[38;2;255;255;255;48;2;140;162;82m    \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m   \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214mCPU 3\u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m    \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;214;97;107m            \u001b[0m\u001b[38;2;255;255;255;48;2;140;162;82m            \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;214;97;107m            \u001b[0m\u001b[38;2;255;255;255;48;2;140;162;82m            \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m            \u001b[0m\u001b[38;2;255;255;255;48;2;214;97;107m            \u001b[0m\u001b[38;2;255;255;255;48;2;140;162;82m            \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m            \u001b[0m\n",
              "\u001b[38;2;0;0;0;48;2;231;203;148m            \u001b[0m\u001b[38;2;255;255;255;48;2;107;110;207m            \u001b[0m\u001b[38;2;255;255;255;48;2;165;81;148m            \u001b[0m\u001b[38;2;255;255;255;48;2;140;109;49m            \u001b[0m\n",
              "\u001b[38;2;0;0;0;48;2;231;203;148m            \u001b[0m\u001b[38;2;255;255;255;48;2;107;110;207m            \u001b[0m\u001b[38;2;255;255;255;48;2;165;81;148m            \u001b[0m\u001b[38;2;255;255;255;48;2;140;109;49m            \u001b[0m\n",
              "\u001b[38;2;0;0;0;48;2;231;203;148m   \u001b[0m\u001b[38;2;0;0;0;48;2;231;203;148mCPU 4\u001b[0m\u001b[38;2;0;0;0;48;2;231;203;148m    \u001b[0m\u001b[38;2;255;255;255;48;2;107;110;207m   \u001b[0m\u001b[38;2;255;255;255;48;2;107;110;207mCPU 5\u001b[0m\u001b[38;2;255;255;255;48;2;107;110;207m    \u001b[0m\u001b[38;2;255;255;255;48;2;165;81;148m   \u001b[0m\u001b[38;2;255;255;255;48;2;165;81;148mCPU 6\u001b[0m\u001b[38;2;255;255;255;48;2;165;81;148m    \u001b[0m\u001b[38;2;255;255;255;48;2;140;109;49m   \u001b[0m\u001b[38;2;255;255;255;48;2;140;109;49mCPU 7\u001b[0m\u001b[38;2;255;255;255;48;2;140;109;49m    \u001b[0m\n",
              "\u001b[38;2;0;0;0;48;2;231;203;148m            \u001b[0m\u001b[38;2;255;255;255;48;2;107;110;207m            \u001b[0m\u001b[38;2;255;255;255;48;2;165;81;148m            \u001b[0m\u001b[38;2;255;255;255;48;2;140;109;49m            \u001b[0m\n",
              "\u001b[38;2;0;0;0;48;2;231;203;148m            \u001b[0m\u001b[38;2;255;255;255;48;2;107;110;207m            \u001b[0m\u001b[38;2;255;255;255;48;2;165;81;148m            \u001b[0m\u001b[38;2;255;255;255;48;2;140;109;49m            \u001b[0m\n",
              "\u001b[38;2;0;0;0;48;2;231;203;148m            \u001b[0m\u001b[38;2;255;255;255;48;2;107;110;207m            \u001b[0m\u001b[38;2;255;255;255;48;2;165;81;148m            \u001b[0m\u001b[38;2;255;255;255;48;2;140;109;49m            \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #d6616b\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8ca252\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #d6616b\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8ca252\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">   CPU 0    </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #d6616b\">   CPU 1    </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8ca252\">   CPU 2    </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">   CPU 3    </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #d6616b\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8ca252\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #d6616b\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8ca252\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #d6616b\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8ca252\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">            </span>\n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #e7cb94\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #6b6ecf\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #a55194\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8c6d31\">            </span>\n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #e7cb94\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #6b6ecf\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #a55194\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8c6d31\">            </span>\n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #e7cb94\">   CPU 4    </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #6b6ecf\">   CPU 5    </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #a55194\">   CPU 6    </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8c6d31\">   CPU 7    </span>\n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #e7cb94\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #6b6ecf\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #a55194\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8c6d31\">            </span>\n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #e7cb94\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #6b6ecf\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #a55194\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8c6d31\">            </span>\n",
              "<span style=\"color: #000000; text-decoration-color: #000000; background-color: #e7cb94\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #6b6ecf\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #a55194\">            </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #8c6d31\">            </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Automatic parallelism via jit"
      ],
      "metadata": {
        "id": "5LGavJcMry1v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you have sharded data, the easiest way to do parallel computation is to simply pass the data to a jax.jit()-compiled function! In JAX, you need to only specify how you want the input and output of your code to be partitioned, and the compiler will figure out how to: 1) partition everything inside; and 2) compile inter-device communications.\n",
        "\n",
        "The XLA compiler behind jit includes heuristics for optimizing computations across multiple devices. In the simplest of cases, those heuristics boil down to computation follows data.\n",
        "\n",
        "To demonstrate how auto-parallelization works in JAX, below is an example that uses a jax.jit()-decorated staged-out function: it’s a simple element-wise function, where the computation for each shard will be performed on the device associated with that shard, and the output is sharded in the same way:"
      ],
      "metadata": {
        "id": "AK7Dq60yr2L2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.jit\n",
        "def f_elementwise(x):\n",
        "  return 2 * jnp.sin(x) + 1\n",
        "\n",
        "result = f_elementwise(arr_sharded)\n",
        "\n",
        "print(\"shardings match:\", result.sharding == arr_sharded.sharding)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJU1gYVhr3je",
        "outputId": "b9ab91e8-581a-4dfd-d4b8-ea4e18d207fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shardings match: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As computations get more complex, the compiler makes decisions about how to best propagate the sharding of the data.\n",
        "\n",
        "Here, you sum along the leading axis of x, and visualize how the result values are stored across multiple devices (with jax.debug.visualize_array_sharding()):"
      ],
      "metadata": {
        "id": "qHuK52PVtJoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.jit\n",
        "def f_contract(x):\n",
        "  return x.sum(axis=0)\n",
        "\n",
        "result = f_contract(arr_sharded)\n",
        "jax.debug.visualize_array_sharding(result)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "slRexOePtKpm",
        "outputId": "177438ad-bd7e-4429-d859-bd6026e2b424"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;255;255;255;48;2;57;59;121m \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121mCPU 0,4\u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214mCPU 1,5\u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m \u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74m \u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74mCPU 2,6\u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74m \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107mCPU 3,7\u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m \u001b[0m\n",
              "\u001b[38;2;255;255;255;48;2;57;59;121m         \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m         \u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74m         \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m         \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\"> CPU 0,4 </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\"> CPU 1,5 </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\"> CPU 2,6 </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\"> CPU 3,7 </span>\n",
              "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">         </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">         </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[48. 52. 56. 60. 64. 68. 72. 76.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The result is partially replicated: that is, the first two elements of the array are replicated on devices 0 and 4, the second on 1 and 5, and so on."
      ],
      "metadata": {
        "id": "794EvfDItkoV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Explicit sharding"
      ],
      "metadata": {
        "id": "o7tkRhzexKMU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main idea behind explicit shardings, (a.k.a. sharding-in-types), is that the JAX-level type of a value includes a description of how the value is sharded. We can query the JAX-level type of any JAX value (or Numpy array, or Python scalar) using jax.typeof:"
      ],
      "metadata": {
        "id": "dgGfvU9-xMvE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "some_array = np.arange(8)\n",
        "print(f\"JAX-level type of some_array: {jax.typeof(some_array)}\")"
      ],
      "metadata": {
        "id": "JTTnch2kxSMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importantly, we can query the type even while tracing under a jit (the JAX-level type is almost defined as “the information about a value we have access to while under a jit)."
      ],
      "metadata": {
        "id": "gAzhNfmvzfaV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.jit\n",
        "def foo(x):\n",
        "  print(f\"JAX-level type of x during tracing: {jax.typeof(x)}\")\n",
        "  return x + x\n",
        "\n",
        "foo(some_array)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "9e5Hpxy3zhMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To start seeing shardings in the type we need to set up an explicit-sharding mesh."
      ],
      "metadata": {
        "id": "HSrlDOvQ0ZRb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from jax.sharding import AxisType\n",
        "\n",
        "mesh = jax.make_mesh((2, 4), (\"X\", \"Y\"),\n",
        "                     axis_types=(AxisType.Explicit, AxisType.Explicit))"
      ],
      "metadata": {
        "id": "HL2mLEqL0ay7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can create some sharded arrays:"
      ],
      "metadata": {
        "id": "O89MmKD21t-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "replicated_array = np.arange(8).reshape(4, 2)\n",
        "sharded_array = jax.device_put(replicated_array, jax.NamedSharding(mesh, P(\"X\", None)))\n",
        "\n",
        "print(f\"replicated_array type: {jax.typeof(replicated_array)}\")\n",
        "print(f\"sharded_array type: {jax.typeof(sharded_array)}\")"
      ],
      "metadata": {
        "id": "qTtQFuaO1u8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We should read the type int32[4@X, 2] as “a 4-by-2 array of 32-bit ints whose first dimension is sharded along mesh axis ‘X’. The array is replicated along all other mesh axes”\n",
        "\n",
        "These shardings associated with JAX-level types propagate through operations. For example:"
      ],
      "metadata": {
        "id": "H_W8G6lI13cU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "arg0 = jax.device_put(np.arange(4).reshape(4, 1),\n",
        "                      jax.NamedSharding(mesh, P(\"X\", None)))\n",
        "arg1 = jax.device_put(np.arange(8).reshape(1, 8),\n",
        "                      jax.NamedSharding(mesh, P(None, \"Y\")))\n",
        "\n",
        "@jax.jit\n",
        "def add_arrays(x, y):\n",
        "  ans = x + y\n",
        "  print(f\"x sharding: {jax.typeof(x)}\")\n",
        "  print(f\"y sharding: {jax.typeof(y)}\")\n",
        "  print(f\"ans sharding: {jax.typeof(ans)}\")\n",
        "  return ans\n",
        "\n",
        "with jax.sharding.use_mesh(mesh):\n",
        "  add_arrays(arg0, arg1)"
      ],
      "metadata": {
        "id": "s5QEWk2u14cZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Manual parallelism with shard_map"
      ],
      "metadata": {
        "id": "d23HkKJIyWWD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the automatic parallelism methods explored above, you can write a function as if you’re operating on the full dataset, and jit will split that computation across multiple devices. By contrast, with jax.shard_map() you write the function that will handle a single shard of data, and shard_map will construct the full function.\n",
        "\n",
        "shard_map works by mapping a function across a particular mesh of devices (shard_map maps over shards). In the example below:\n",
        "\n",
        "1. As before, jax.sharding.Mesh allows for precise device placement, with the axis names parameter for logical and physical axis names.\n",
        "\n",
        "2. The in_specs argument determines the shard sizes. The out_specs argument identifies how the blocks are assembled back together.\n",
        "\n",
        "Note: jax.shard_map() code can work inside jax.jit() if you need it."
      ],
      "metadata": {
        "id": "QlrpzMaqzUUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mesh = jax.make_mesh((8,), ('x',))\n",
        "\n",
        "f_elementwise_sharded = jax.shard_map( # it seems that shard_map is no longer available\n",
        "    f_elementwise,\n",
        "    mesh=mesh,\n",
        "    in_specs=P('x'),\n",
        "    out_specs=P('x'))\n",
        "\n",
        "arr = jnp.arange(32)\n",
        "f_elementwise_sharded(arr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "t_9oxSGczXIi",
        "outputId": "643b2ebd-158d-49c9-fdd4-3a19e3b58272"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'jax' has no attribute 'shard_map'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4-4109912318.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmesh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_mesh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m f_elementwise_sharded = jax.shard_map(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mf_elementwise\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmesh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmesh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/deprecations.py\u001b[0m in \u001b[0;36mgetattr\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     55\u001b[0m       \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeprecationWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"module {module!r} has no attribute {name!r}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'jax' has no attribute 'shard_map'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function you write only “sees” a single batch of the data, which you can check by printing the device local shape:"
      ],
      "metadata": {
        "id": "Gs8E_92F0FjR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = jnp.arange(32)\n",
        "print(f\"global shape: {x.shape=}\")\n",
        "\n",
        "def f(x):\n",
        "  print(f\"device local shape: {x.shape=}\")\n",
        "  return x * 2\n",
        "\n",
        "y = jax.shard_map(f, mesh=mesh, in_specs=P('x'), out_specs=P('x'))(x)"
      ],
      "metadata": {
        "id": "m5cCWZCC0F8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because each of your functions only “sees” the device-local part of the data, it means that aggregation-like functions require some extra thought.\n",
        "\n",
        "For example, here’s what a shard_map of a jax.numpy.sum() looks like:"
      ],
      "metadata": {
        "id": "VLIaEfLf0IJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f(x):\n",
        "  return jnp.sum(x, keepdims=True)\n",
        "\n",
        "jax.shard_map(f, mesh=mesh, in_specs=P('x'), out_specs=P('x'))(x)"
      ],
      "metadata": {
        "id": "PHy8fL0h0JSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your function f operates separately on each shard, and the resulting summation reflects this.\n",
        "\n",
        "If you want to sum across shards, you need to explicitly request it using collective operations like jax.lax.psum():"
      ],
      "metadata": {
        "id": "-eqX56Ww0Ks6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f(x):\n",
        "  sum_in_shard = x.sum()\n",
        "  return jax.lax.psum(sum_in_shard, 'x')\n",
        "\n",
        "jax.shard_map(f, mesh=mesh, in_specs=P('x'), out_specs=P())(x)"
      ],
      "metadata": {
        "id": "D3CF8OWC0LnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because the output no longer has a sharded dimension, set out_specs=P() (recall that the out_specs argument identifies how the blocks are assembled back together in shard_map)."
      ],
      "metadata": {
        "id": "bqUGt-Gi0NrK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparing the three approaches"
      ],
      "metadata": {
        "id": "Q8KojxL80PSC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With these concepts fresh in our mind, let’s compare the three approaches for a simple neural network layer.\n",
        "\n",
        "Start by defining your canonical function like this:"
      ],
      "metadata": {
        "id": "icOKB7n-0rV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.jit\n",
        "def layer(x, weights, bias):\n",
        "  return jax.nn.sigmoid(x @ weights + bias)"
      ],
      "metadata": {
        "id": "oZygiLjf1Tyh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "rng = np.random.default_rng(0)\n",
        "\n",
        "x = rng.normal(size=(32,))\n",
        "weights = rng.normal(size=(32, 4))\n",
        "bias = rng.normal(size=(4,))\n",
        "\n",
        "layer(x, weights, bias)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sk-0oYCq1VY5",
        "outputId": "d2d2d002-76b5-488a-b99e-ec7c35a65209"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([0.02138916, 0.8931118 , 0.5989196 , 0.9774251 ], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can automatically run this in a distributed manner using jax.jit() and passing appropriately sharded data.\n",
        "\n",
        "If you shard the leading axis of both x and make weights fully replicated, then the matrix multiplication will automatically happen in parallel:"
      ],
      "metadata": {
        "id": "LbFC-S9m1XKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mesh = jax.make_mesh((8,), ('x',))\n",
        "x_sharded = jax.device_put(x, jax.NamedSharding(mesh, P('x')))\n",
        "weights_sharded = jax.device_put(weights, jax.NamedSharding(mesh, P()))\n",
        "\n",
        "layer(x_sharded, weights_sharded, bias)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdCpy4RX12ix",
        "outputId": "a729f97d-e74c-4ee1-d230-72e75e8575e9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([0.02138916, 0.8931118 , 0.5989196 , 0.9774251 ], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternatively, you can use explicit sharding mode too:"
      ],
      "metadata": {
        "id": "PeiJaRhj2J0J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "explicit_mesh = jax.make_mesh((8,), ('X',), axis_types=(AxisType.Explicit,))\n",
        "\n",
        "x_sharded = jax.device_put(x, jax.NamedSharding(explicit_mesh, P('X')))\n",
        "weights_sharded = jax.device_put(weights, jax.NamedSharding(explicit_mesh, P()))\n",
        "\n",
        "@jax.jit\n",
        "def layer_auto(x, weights, bias):\n",
        "  print(f\"x sharding: {jax.typeof(x)}\")\n",
        "  print(f\"weights sharding: {jax.typeof(weights)}\")\n",
        "  print(f\"bias sharding: {jax.typeof(bias)}\")\n",
        "  out = layer(x, weights, bias)\n",
        "  print(f\"out sharding: {jax.typeof(out)}\")\n",
        "  return out\n",
        "\n",
        "with jax.sharding.use_mesh(explicit_mesh):\n",
        "  layer_auto(x_sharded, weights_sharded, bias)"
      ],
      "metadata": {
        "id": "ixH8zeEC2K0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, you can do the same thing with shard_map, using jax.lax.psum() to indicate the cross-shard collective required for the matrix product:"
      ],
      "metadata": {
        "id": "FCgJNhiV2O7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "\n",
        "@jax.jit\n",
        "@partial(jax.shard_map, mesh=mesh,\n",
        "         in_specs=(P('x'), P('x', None), P(None)),\n",
        "         out_specs=P(None))\n",
        "def layer_sharded(x, weights, bias):\n",
        "  return jax.nn.sigmoid(jax.lax.psum(x @ weights, 'x') + bias)\n",
        "\n",
        "layer_sharded(x, weights, bias)"
      ],
      "metadata": {
        "id": "aJ2wjHB02QCR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}