{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8xXuZF9oHAv",
        "outputId": "f744422f-ffe2-42be-cfa2-811b95d8e59d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: jax in /usr/local/lib/python3.11/dist-packages (0.5.2)\n",
            "Requirement already satisfied: jaxlib<=0.5.2,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from jax) (0.5.1)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from jax) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.25 in /usr/local/lib/python3.11/dist-packages (from jax) (2.0.2)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.11/dist-packages (from jax) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from jax) (1.15.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install jax"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Jax as numpy"
      ],
      "metadata": {
        "id": "kwbt6rAXrKnD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# jax as numpy\n",
        "import jax.numpy as jnp\n",
        "\n",
        "def selu(x, alpha=1.67, lmbda=1.05):\n",
        "  return lmbda * jnp.where(x > 0, x, alpha * jnp.exp(x) - alpha)\n",
        "\n",
        "x = jnp.arange(5.0)\n",
        "print(x)\n",
        "print(selu(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LhaJo4tZpdQj",
        "outputId": "187a4a2e-da8b-4b3b-be6a-ea48d2a722d1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 1. 2. 3. 4.]\n",
            "[0.        1.05      2.1       3.1499999 4.2      ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Just-in-time compilation with jax.jit()"
      ],
      "metadata": {
        "id": "rQnSqgwlrRty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Without jit\n",
        "from jax import random\n",
        "key = random.key(1701)\n",
        "x = random.normal(key, (1_000_000,))\n",
        "%timeit selu(x).block_until_ready()\n",
        "\n",
        "# With jit\n",
        "from jax import jit\n",
        "selu_jit = jit(selu)\n",
        "_ = selu_jit(x)  # compiles on first call\n",
        "%timeit selu_jit(x).block_until_ready()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bI1BM1jeqSY7",
        "outputId": "c2c16551-2160-4265-8834-e17859664a69"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.51 ms ± 668 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
            "982 µs ± 7.87 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Taking derivatives with jax.grad()"
      ],
      "metadata": {
        "id": "fWxQKnJrsINh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from jax import grad\n",
        "\n",
        "def sum_logistic(x):\n",
        "  return jnp.sum(1.0 / (1.0 + jnp.exp(-x)))\n",
        "\n",
        "x_small = jnp.arange(3.)\n",
        "print(x_small)\n",
        "derivative_fn = grad(sum_logistic) # this is the gradient/derivative function of the function sum_logistic\n",
        "print(derivative_fn(x_small)) # for each element in the array x_small, compute its value"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_AmNT8-sLy5",
        "outputId": "559b7bba-b794-4130-c1fb-df2fc613989c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 1. 2.]\n",
            "[0.25       0.19661197 0.10499357]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's verify with finite differences that our result is correct."
      ],
      "metadata": {
        "id": "k9y03-UYudYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def first_finite_differences(f, x, eps=1E-3):\n",
        "  return jnp.array([(f(x + eps * v) - f(x - eps * v)) / (2 * eps)\n",
        "                   for v in jnp.eye(len(x))])\n",
        "\n",
        "print(first_finite_differences(sum_logistic, x_small))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIk5NtDLuhkg",
        "outputId": "b3923754-0232-474e-b4d1-d77ddf7d840d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.24998187 0.1964569  0.10502338]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The grad() and jit() transformations compose and can be mixed arbitrarily. For instance, while the sum_logistic function was differentiated directly in the previous example, it could also be JIT-compiled, and these operations can be combined. We can go further:"
      ],
      "metadata": {
        "id": "mfbSeL-4uzKg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(grad(jit(grad(jit(grad(sum_logistic)))))(1.0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAc_EU82u3jI",
        "outputId": "7e5ad7b6-68f3-4e86-f00d-a889bbd6154e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-0.0353256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Full Jacobian matrix for vector-valued functions"
      ],
      "metadata": {
        "id": "6fRZ185YvGpg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from jax import jacobian\n",
        "print(jacobian(jnp.exp)(x_small))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hovywqFovMt4",
        "outputId": "6a0d64d9-87c9-4405-d18c-571c69c69681"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.        0.        0.       ]\n",
            " [0.        2.7182817 0.       ]\n",
            " [0.        0.        7.389056 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For more advanced autodiff operations, you can use jax.vjp() for reverse-mode vector-Jacobian products, and jax.jvp() and jax.linearize() for forward-mode Jacobian-vector products. The two can be composed arbitrarily with one another, and with other JAX transformations. For example, jax.jvp() and jax.vjp() are used to define the forward-mode jax.jacfwd() and reverse-mode jax.jacrev() for computing Jacobians in forward- and reverse-mode, respectively. Here’s one way to compose them to make a function that efficiently computes full Hessian matrices:"
      ],
      "metadata": {
        "id": "3B6VhYfMvjlH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from jax import jacfwd, jacrev\n",
        "def hessian(fun):\n",
        "  return jit(jacfwd(jacrev(fun)))\n",
        "print(hessian(sum_logistic)(x_small))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mk2dNHv8vlg3",
        "outputId": "c9c5c24a-6938-434b-dccd-0fc731a43f87"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.         -0.         -0.        ]\n",
            " [-0.         -0.09085776 -0.        ]\n",
            " [-0.         -0.         -0.07996249]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Auto-vectorization with jax.vmap()"
      ],
      "metadata": {
        "id": "iQBZY6-hxkV2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another useful transformation is vmap(), the vectorizing map. It has the familiar semantics of mapping a function along array axes, but instead of explicitly looping over function calls, it transforms the function into a natively vectorized version for better performance. When composed with jit(), it can be just as performant as manually rewriting your function to operate over an extra batch dimension."
      ],
      "metadata": {
        "id": "8XaTL0vqyaOG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "key1, key2 = random.split(key)\n",
        "mat = random.normal(key1, (150, 100))\n",
        "batched_x = random.normal(key2, (10, 100))\n",
        "print(\"key1:\", key1)\n",
        "print(\"key2:\", key2)\n",
        "print(\"mat:\", mat)\n",
        "print(\"batched_x:\", batched_x)\n",
        "\n",
        "def apply_matrix(x):\n",
        "  return jnp.dot(mat, x)"
      ],
      "metadata": {
        "id": "axiB3tw9yeOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The apply_matrix function maps a vector to a vector, but we may want to apply it row-wise across a matrix. We could do this by looping over the batch dimension in Python, but this usually results in poor performance."
      ],
      "metadata": {
        "id": "SE_h6P30zygm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def naively_batched_apply_matrix(v_batched):\n",
        "  return jnp.stack([apply_matrix(v) for v in v_batched])\n",
        "\n",
        "print('Naively batched')\n",
        "%timeit naively_batched_apply_matrix(batched_x).block_until_ready()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "po1Cfspcz0j8",
        "outputId": "3a162daa-4f6a-47f9-fa2d-475464132335"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naively batched\n",
            "2.93 ms ± 466 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of using matrix * vector and stack them up, we can directly compute matrix * matrix to get our final answer matrix."
      ],
      "metadata": {
        "id": "pKlc08hZ2VB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "@jit\n",
        "def batched_apply_matrix(batched_x):\n",
        "  return jnp.dot(batched_x, mat.T)\n",
        "\n",
        "np.testing.assert_allclose(naively_batched_apply_matrix(batched_x),\n",
        "                           batched_apply_matrix(batched_x), atol=1E-4, rtol=1E-4)\n",
        "print('Manually batched')\n",
        "%timeit batched_apply_matrix(batched_x).block_until_ready()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUlTTAKm2u6b",
        "outputId": "d1a0a81d-dd59-4b8a-cf89-4c145c6888e8"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Manually batched\n",
            "37.6 µs ± 2.37 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, as functions become more complicated, this kind of manual batching becomes more difficult and error-prone. The vmap() transformation is designed to automatically transform a function into a batch-aware version:"
      ],
      "metadata": {
        "id": "tpVmy78R21Ws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from jax import vmap\n",
        "\n",
        "@jit\n",
        "def vmap_batched_apply_matrix(batched_x):\n",
        "  return vmap(apply_matrix)(batched_x)\n",
        "\n",
        "np.testing.assert_allclose(naively_batched_apply_matrix(batched_x),\n",
        "                           vmap_batched_apply_matrix(batched_x), atol=1E-4, rtol=1E-4)\n",
        "print('Auto-vectorized with vmap')\n",
        "%timeit vmap_batched_apply_matrix(batched_x).block_until_ready()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cnmmTuv3228c",
        "outputId": "914dffad-611a-4df9-aba8-f84288d3dbd0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Auto-vectorized with vmap\n",
            "50.4 µs ± 2.28 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you would expect, vmap() can be arbitrarily composed with jit(), grad(), and any other JAX transformation."
      ],
      "metadata": {
        "id": "9urUp7vy3IsE"
      }
    }
  ]
}